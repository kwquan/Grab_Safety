{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n"
     ]
    }
   ],
   "source": [
    "#IMPORT MODULES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib.pyplot import figure\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import shap\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PRE-PROCESSING\n",
    "#Read the 10 training folders \n",
    "feat_1 = pd.read_csv('features_1')\n",
    "feat_2 = pd.read_csv('features_2')\n",
    "feat_3 = pd.read_csv('features_3')\n",
    "feat_4 = pd.read_csv('features_4')\n",
    "feat_5 = pd.read_csv('features_5')\n",
    "feat_6 = pd.read_csv('features_6')\n",
    "feat_7 = pd.read_csv('features_7')\n",
    "feat_8 = pd.read_csv('features_8')\n",
    "feat_9 = pd.read_csv('features_9')\n",
    "feat_10 = pd.read_csv('features_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookingID</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Bearing</th>\n",
       "      <th>acceleration_x</th>\n",
       "      <th>acceleration_y</th>\n",
       "      <th>acceleration_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>second</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1202590843006</td>\n",
       "      <td>3.000</td>\n",
       "      <td>353.0</td>\n",
       "      <td>1.228867</td>\n",
       "      <td>8.900100</td>\n",
       "      <td>3.986968</td>\n",
       "      <td>0.008221</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>-0.009966</td>\n",
       "      <td>1362.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274877907034</td>\n",
       "      <td>9.293</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.032775</td>\n",
       "      <td>8.659933</td>\n",
       "      <td>4.737300</td>\n",
       "      <td>0.024629</td>\n",
       "      <td>0.004028</td>\n",
       "      <td>-0.010858</td>\n",
       "      <td>257.0</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>884763263056</td>\n",
       "      <td>3.000</td>\n",
       "      <td>189.0</td>\n",
       "      <td>1.139675</td>\n",
       "      <td>9.545974</td>\n",
       "      <td>1.951334</td>\n",
       "      <td>-0.006899</td>\n",
       "      <td>-0.015080</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>973.0</td>\n",
       "      <td>0.667059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1073741824054</td>\n",
       "      <td>3.900</td>\n",
       "      <td>126.0</td>\n",
       "      <td>3.871543</td>\n",
       "      <td>10.386364</td>\n",
       "      <td>-0.136474</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>-0.339601</td>\n",
       "      <td>-0.017956</td>\n",
       "      <td>902.0</td>\n",
       "      <td>7.913285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1056561954943</td>\n",
       "      <td>3.900</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-0.112882</td>\n",
       "      <td>10.550960</td>\n",
       "      <td>-1.560110</td>\n",
       "      <td>0.130568</td>\n",
       "      <td>-0.061697</td>\n",
       "      <td>0.161530</td>\n",
       "      <td>820.0</td>\n",
       "      <td>20.419409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bookingID  Accuracy  Bearing  acceleration_x  acceleration_y  \\\n",
       "0  1202590843006     3.000    353.0        1.228867        8.900100   \n",
       "1   274877907034     9.293     17.0        0.032775        8.659933   \n",
       "2   884763263056     3.000    189.0        1.139675        9.545974   \n",
       "3  1073741824054     3.900    126.0        3.871543       10.386364   \n",
       "4  1056561954943     3.900     50.0       -0.112882       10.550960   \n",
       "\n",
       "   acceleration_z    gyro_x    gyro_y    gyro_z  second      Speed  \n",
       "0        3.986968  0.008221  0.002269 -0.009966  1362.0   0.000000  \n",
       "1        4.737300  0.024629  0.004028 -0.010858   257.0   0.190000  \n",
       "2        1.951334 -0.006899 -0.015080  0.001122   973.0   0.667059  \n",
       "3       -0.136474  0.001344 -0.339601 -0.017956   902.0   7.913285  \n",
       "4       -1.560110  0.130568 -0.061697  0.161530   820.0  20.419409  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA PRE-PROCESSING\n",
    "#combine the 10 training folders into 1 dataframe \n",
    "train_X = feat_1.append(feat_2)\n",
    "train_X = train_X.append(feat_3)\n",
    "train_X = train_X.append(feat_4)\n",
    "train_X = train_X.append(feat_5)\n",
    "train_X = train_X.append(feat_6)\n",
    "train_X = train_X.append(feat_7)\n",
    "train_X = train_X.append(feat_8)\n",
    "train_X = train_X.append(feat_9)\n",
    "train_X = train_X.append(feat_10)\n",
    "train_X = train_X.reset_index(drop=True)\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PRE-PROCESSING(FOR TEST SET)\n",
    "#REPLACE xxxx with file name\n",
    "#This is for reading the hold-out test file \n",
    "test_X = pd.read_csv(xxx)\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PRE-PROCESSING\n",
    "#Checking rows of data\n",
    "len(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PRE-PROCESSING\n",
    "#Checking number of unique rides\n",
    "train_X.bookingID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PRE-PROCESSING\n",
    "#Sort by unique rides, from start to end of trip\n",
    "train_X = train_X.sort_values(['bookingID','second'])\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookingID</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Bearing</th>\n",
       "      <th>acceleration_x</th>\n",
       "      <th>acceleration_y</th>\n",
       "      <th>acceleration_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>second</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10835302</th>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>143.298294</td>\n",
       "      <td>0.818112</td>\n",
       "      <td>-9.941461</td>\n",
       "      <td>-2.014999</td>\n",
       "      <td>-0.016245</td>\n",
       "      <td>-0.094040</td>\n",
       "      <td>0.070732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.442991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12007854</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>143.298294</td>\n",
       "      <td>0.546405</td>\n",
       "      <td>-9.835590</td>\n",
       "      <td>-2.038925</td>\n",
       "      <td>-0.047092</td>\n",
       "      <td>-0.078874</td>\n",
       "      <td>0.043187</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3394723</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>143.298294</td>\n",
       "      <td>-1.706207</td>\n",
       "      <td>-9.270792</td>\n",
       "      <td>-1.209448</td>\n",
       "      <td>-0.028965</td>\n",
       "      <td>-0.032652</td>\n",
       "      <td>0.015390</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.228454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436147</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>143.298294</td>\n",
       "      <td>-1.416705</td>\n",
       "      <td>-9.548032</td>\n",
       "      <td>-1.860977</td>\n",
       "      <td>-0.022413</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>-0.025753</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.228454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9490986</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>143.298294</td>\n",
       "      <td>-0.598145</td>\n",
       "      <td>-9.853534</td>\n",
       "      <td>-1.378574</td>\n",
       "      <td>-0.014297</td>\n",
       "      <td>-0.046206</td>\n",
       "      <td>0.021902</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.228454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bookingID  Accuracy     Bearing  acceleration_x  acceleration_y  \\\n",
       "10835302          0      12.0  143.298294        0.818112       -9.941461   \n",
       "12007854          0       8.0  143.298294        0.546405       -9.835590   \n",
       "3394723           0       8.0  143.298294       -1.706207       -9.270792   \n",
       "436147            0       8.0  143.298294       -1.416705       -9.548032   \n",
       "9490986           0       8.0  143.298294       -0.598145       -9.853534   \n",
       "\n",
       "          acceleration_z    gyro_x    gyro_y    gyro_z  second     Speed  \n",
       "10835302       -2.014999 -0.016245 -0.094040  0.070732     0.0  3.442991  \n",
       "12007854       -2.038925 -0.047092 -0.078874  0.043187     1.0  0.228454  \n",
       "3394723        -1.209448 -0.028965 -0.032652  0.015390     2.0  0.228454  \n",
       "436147         -1.860977 -0.022413  0.005049 -0.025753     3.0  0.228454  \n",
       "9490986        -1.378574 -0.014297 -0.046206  0.021902     4.0  0.228454  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA PRE-PROCESSING(FOR TEST SET)\n",
    "#Sort by unique rides, from start to end of trip \n",
    "#Note that unlike train_X, test_X index is jumbled up\n",
    "test_X = test_X.sort_values(['bookingID','second'])\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookingID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111669149733</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>335007449205</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171798691856</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1520418422900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>798863917116</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bookingID  label\n",
       "0   111669149733      0\n",
       "1   335007449205      1\n",
       "2   171798691856      0\n",
       "3  1520418422900      0\n",
       "4   798863917116      0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA PRE-PROCESSING\n",
    "#Read file containing labels for each ride \n",
    "train_y = pd.read_csv('labels')\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PRE-PROCESSING\n",
    "#Merge labels with rides so that we can look at the variable distributions for each driver type later\n",
    "train_X = train_X.merge(train_y, on='bookingID', how='left')\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PRE-PROCESSING\n",
    "#Separate training set into dangerous and safe drivers[0=safe,1=dangerous]\n",
    "train_X_0 = train_X[train_X['label'] == 0]\n",
    "train_X_1 = train_X[train_X['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PRE-PROCESSING\n",
    "#Checking first 5 rows of safe drivers set\n",
    "train_X_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PRE-PROCESSING\n",
    "#Checking first 5 rows of dangerous drivers set\n",
    "train_X_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Speed distribution for safe & dangerous drivers\n",
    "figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "sns.distplot(train_X_0['Speed'],hist = False,label='Safe')\n",
    "sns.distplot(train_X_1['Speed'],hist = False,label='Dangerous')\n",
    "plt.xlabel('Speed',fontsize=9)\n",
    "locs,labels = plt.xticks()\n",
    "plt.tick_params(axis='x',which='major',labelsize=6,pad=-6)\n",
    "plt.tick_params(axis='y',which='major',labelsize=6)\n",
    "plt.show() \n",
    "#We see more outlier on the right side for dangerous drivers\n",
    "#Speed for dangerous drivers is more erratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check speed outliers for both driver types\n",
    "train_X.boxplot(by='label', column=['Speed'], grid=False)\n",
    "#As seen in the boxplot, dangerous drivers have more outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary speed stats for safe drivers \n",
    "train_X_0['Speed'].describe()\n",
    "#Notice that negative speed values exist\n",
    "#This may be measurement errors due to GPS inaccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary speed stats for dangerous drivers \n",
    "train_X_1['Speed'].describe()\n",
    "#Negative speed values exist for dangerous drivers as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Let's plot distribution of negative speed values against GPS accuracy for negative speed occurences only\n",
    "figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "sns.distplot(train_X[train_X['Speed'] < 0]['Accuracy'],hist = False,label='Accuracy for negative speed')\n",
    "plt.xlabel('Accuracy',fontsize=9)\n",
    "locs,labels = plt.xticks()\n",
    "plt.tick_params(axis='x',which='major',labelsize=6,pad=-6)\n",
    "plt.tick_params(axis='y',which='major',labelsize=6)\n",
    "plt.show() \n",
    "#Notice the spike of high accuracy values \n",
    "#Seems that negative speed values are the result of GPS inaccuracy (denoted by the large accuracy values) \n",
    "#Need to verify for positive speed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Let's plot distribution of GPS accuracy for positive speed occurences only\n",
    "train_X_pos = train_X[train_X['Speed'] > 0]\n",
    "figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "sns.distplot(train_X_pos['Accuracy'],hist = False,label='Accuracy')\n",
    "plt.xlabel('Accuracy',fontsize=9)\n",
    "locs,labels = plt.xticks()\n",
    "plt.tick_params(axis='x',which='major',labelsize=6,pad=-6)\n",
    "plt.tick_params(axis='y',which='major',labelsize=6)\n",
    "plt.show() \n",
    "#As expected, most of them are concentrated in the region where accuracy values are low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Let's plot distribution of accuracy values for all speed values\n",
    "#Note that this is a combination of the above 2 plots\n",
    "figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "sns.distplot(train_X['Accuracy'],hist = False,label='Accuracy')\n",
    "plt.xlabel('Accuracy',fontsize=9)\n",
    "locs,labels = plt.xticks()\n",
    "plt.tick_params(axis='x',which='major',labelsize=6,pad=-6)\n",
    "plt.tick_params(axis='y',which='major',labelsize=6)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary accuracy stats for all drivers \n",
    "train_X['Accuracy'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Plot gyro_z distribution for safe & dangerous drivers\n",
    "figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(2,1,2)\n",
    "sns.distplot(train_X_0['gyro_z'],hist = False,label='gyro_z for safe drivers')\n",
    "sns.distplot(train_X_1['gyro_z'],hist = False,label='gyro_z for dangerous drivers')\n",
    "plt.xlabel('gyro_z',fontsize=9)\n",
    "locs,labels = plt.xticks()\n",
    "plt.tick_params(axis='x',which='major',labelsize=6,pad=-6)\n",
    "plt.tick_params(axis='y',which='major',labelsize=6)\n",
    "plt.show() \n",
    "#distribution seems to be the same for both driver types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary gyro_z stats for safe drivers \n",
    "train_X_0['gyro_z'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary accuracy stats for dangerous drivers \n",
    "train_X_1['gyro_z'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Plot gyro_y distribution for safe & dangerous drivers\n",
    "figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(2,1,2)\n",
    "sns.distplot(train_X_0['gyro_y'],hist = False,label='gyro_y for safe drivers')\n",
    "sns.distplot(train_X_1['gyro_y'],hist = False,label='gyro_y for dangerous drivers')\n",
    "plt.xlabel('gyro_y',fontsize=9)\n",
    "locs,labels = plt.xticks()\n",
    "plt.tick_params(axis='x',which='major',labelsize=6,pad=-6)\n",
    "plt.tick_params(axis='y',which='major',labelsize=6)\n",
    "plt.show() \n",
    "#distribution seems to be the same for both driver types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary gyro_y stats for safe drivers \n",
    "train_X_0['gyro_y'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary gyro_y stats for dangerous drivers \n",
    "train_X_1['gyro_y'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Plot gyro_x distribution for safe & dangerous drivers\n",
    "figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(2,1,2)\n",
    "sns.distplot(train_X_0['gyro_x'],hist = False,label='gyro_x for safe drivers')\n",
    "sns.distplot(train_X_1['gyro_x'],hist = False,label='gyro_x for dangerous drivers')\n",
    "plt.xlabel('gyro_x',fontsize=9)\n",
    "locs,labels = plt.xticks()\n",
    "plt.tick_params(axis='x',which='major',labelsize=6,pad=-6)\n",
    "plt.tick_params(axis='y',which='major',labelsize=6)\n",
    "plt.show() \n",
    "#distribution seems to be the same for both driver types\n",
    "#however, values for safe drivers seem to be slightly-higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary gyro_x stats for safe drivers \n",
    "train_X_0['gyro_x'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary gyro_x stats for dangerous drivers \n",
    "train_X_1['gyro_x'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Plot acc_z distribution for safe & dangerous drivers\n",
    "figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(2,1,2)\n",
    "sns.distplot(train_X_0['acceleration_z'],hist = False,label='acc_z for safe drivers')\n",
    "sns.distplot(train_X_1['acceleration_z'],hist = False,label='acc_z for dangerous drivers')\n",
    "plt.xlabel('acc_z',fontsize=9)\n",
    "locs,labels = plt.xticks()\n",
    "plt.tick_params(axis='x',which='major',labelsize=6,pad=-6)\n",
    "plt.tick_params(axis='y',which='major',labelsize=6)\n",
    "plt.show() \n",
    "#distribution seems to be the same for both driver types\n",
    "#however, values for safe drivers seem to be slightly-higher\n",
    "#dangerous drivers seem to have slightly-lower peak but fatter tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary acceleration_z stats for safe drivers \n",
    "train_X_0['acceleration_z'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary acceleration_z stats for dangerous drivers \n",
    "train_X_1['acceleration_z'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Plot acc_y distribution for safe & dangerous drivers\n",
    "figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(2,1,2)\n",
    "sns.distplot(train_X_0['acceleration_y'],hist = False,label='acc_y for safe drivers')\n",
    "sns.distplot(train_X_1['acceleration_y'],hist = False,label='acc_y for dangerous drivers')\n",
    "plt.xlabel('acc_y',fontsize=9)\n",
    "locs,labels = plt.xticks()\n",
    "plt.tick_params(axis='x',which='major',labelsize=6,pad=-6)\n",
    "plt.tick_params(axis='y',which='major',labelsize=6)\n",
    "plt.show() \n",
    "#distribution seems to be the same for both driver types\n",
    "#interesting 2 peaks observed\n",
    "#however, values for safe drivers seem to be slightly-higher\n",
    "#dangerous drivers seem to have slightly-lower peaks, and have values shifted slightly to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary acceleration_y stats for safe drivers \n",
    "train_X_0['acceleration_y'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary acceleration_y stats for dangerous drivers \n",
    "train_X_1['acceleration_y'].describe()\n",
    "#dangerous drivers have higher max,lower min\n",
    "#implies abrupt acc & jam-brakes may be large contributor to driver differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Plot acc_x distribution for safe & dangerous drivers\n",
    "figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(2,1,2)\n",
    "sns.distplot(train_X_0['acceleration_x'],hist = False,label='acc_x for safe drivers')\n",
    "sns.distplot(train_X_1['acceleration_x'],hist = False,label='acc_x for dangerous drivers')\n",
    "plt.xlabel('acc_x',fontsize=9)\n",
    "locs,labels = plt.xticks()\n",
    "plt.tick_params(axis='x',which='major',labelsize=6,pad=-6)\n",
    "plt.tick_params(axis='y',which='major',labelsize=6)\n",
    "plt.show() \n",
    "#distribution seems to be the same for both driver types\n",
    "#however, values for safe drivers seem to be slightly-higher\n",
    "#dangerous drivers seem to have slightly-lower peak but fatter tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary acceleration_x stats for safe drivers \n",
    "train_X_0['acceleration_x'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Check summary acceleration_x stats for dangerous drivers \n",
    "train_X_1['acceleration_x'].describe()\n",
    "#dangerous drivers have higher acc_x std\n",
    "#much bigger max \n",
    "#much lower min\n",
    "#implying abrupt acc & dcc during turns may be large contributor to driver type differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Plot bearing distribution for safe & dangerous drivers\n",
    "figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(2,1,2)\n",
    "sns.distplot(train_X_0['Bearing'],hist = False,label='bearing for safe drivers')\n",
    "sns.distplot(train_X_1['Bearing'],hist = False,label='bearing for dangerous drivers')\n",
    "plt.xlabel('bearing',fontsize=9)\n",
    "locs,labels = plt.xticks()\n",
    "plt.tick_params(axis='x',which='major',labelsize=6,pad=-6)\n",
    "plt.tick_params(axis='y',which='major',labelsize=6)\n",
    "plt.show() \n",
    "#distribution is the same for both driver types\n",
    "#result is expected since we don't expect dangerous drivers to prefer certain routes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Correlation matrix for safe drivers\n",
    "corr = train_X_0.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATORY DATA ANALYSIS\n",
    "#Correlation matrix for dangerous drivers\n",
    "corr = train_X_1.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA PROCESSING\n",
    "#There exist some bookings with both labels\n",
    "#Find such bookings & add to to_drop\n",
    "#we are going to drop these bookings from our training set \n",
    "to_drop = []\n",
    "for i in train_X.bookingID.unique():\n",
    "    if len(train_y[train_y['bookingID'] == i]) > 1:\n",
    "        to_drop.append(i)\n",
    "#check number of bookings with dubious labels        \n",
    "len(to_drop)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Drop rides with dubious labels from both train_X & train_y\n",
    "train_y = train_y[~train_y['bookingID'].isin(to_drop)]\n",
    "train_X = train_X[~train_X['bookingID'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Check for trips with only negative speeds & add to to_drop\n",
    "to_drop = []\n",
    "for i in tqdm(train_X.bookingID.unique()):\n",
    "    if len(train_X[(train_X.bookingID == i) & (train_X.Speed < 0)]) == len(train_X[train_X.bookingID == i]):\n",
    "        to_drop.append(i)\n",
    "#check number of bookings with dubious labels        \n",
    "len(to_drop)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Drop rides with only negative speeds from both train_X & train_y\n",
    "train_y = train_y[~train_y['bookingID'].isin(to_drop)]\n",
    "train_X = train_X[~train_X['bookingID'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Check minimum accuracy for these rides with only negative speed\n",
    "#May use this value as threshold\n",
    "minimum = 100\n",
    "for i in to_drop:\n",
    "    if train_X[train_X.bookingID == i]['Accuracy'].min() < minimum:\n",
    "        minimum = train_X[train_X.bookingID == i]['Accuracy'].min()\n",
    "minimum        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a2bc8413774037acb345d8e1ca7ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19982), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA PROCESSING(FOR TEST SET)\n",
    "#Check for trips with only negative speeds & add to to_drop \n",
    "to_drop = []\n",
    "for i in tqdm(test_X.bookingID.unique()):\n",
    "    if len(test_X[(test_X.bookingID == i) & (test_X.Speed < 0)]) == len(test_X[test_X.bookingID == i]):\n",
    "        to_drop.append(i)\n",
    "#check number of bookings with negative speed       \n",
    "len(to_drop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING(FOR TEST SET)\n",
    "#Drop rides with only negative speeds from test_X\n",
    "test_X = test_X[~test_X['bookingID'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookingID</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Bearing</th>\n",
       "      <th>acceleration_x</th>\n",
       "      <th>acceleration_y</th>\n",
       "      <th>acceleration_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>second</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>143.298294</td>\n",
       "      <td>0.818112</td>\n",
       "      <td>-9.941461</td>\n",
       "      <td>-2.014999</td>\n",
       "      <td>-0.016245</td>\n",
       "      <td>-0.094040</td>\n",
       "      <td>0.070732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.442991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>143.298294</td>\n",
       "      <td>0.546405</td>\n",
       "      <td>-9.835590</td>\n",
       "      <td>-2.038925</td>\n",
       "      <td>-0.047092</td>\n",
       "      <td>-0.078874</td>\n",
       "      <td>0.043187</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>143.298294</td>\n",
       "      <td>-1.706207</td>\n",
       "      <td>-9.270792</td>\n",
       "      <td>-1.209448</td>\n",
       "      <td>-0.028965</td>\n",
       "      <td>-0.032652</td>\n",
       "      <td>0.015390</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.228454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>143.298294</td>\n",
       "      <td>-1.416705</td>\n",
       "      <td>-9.548032</td>\n",
       "      <td>-1.860977</td>\n",
       "      <td>-0.022413</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>-0.025753</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.228454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>143.298294</td>\n",
       "      <td>-0.598145</td>\n",
       "      <td>-9.853534</td>\n",
       "      <td>-1.378574</td>\n",
       "      <td>-0.014297</td>\n",
       "      <td>-0.046206</td>\n",
       "      <td>0.021902</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.228454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bookingID  Accuracy     Bearing  acceleration_x  acceleration_y  \\\n",
       "0          0      12.0  143.298294        0.818112       -9.941461   \n",
       "1          0       8.0  143.298294        0.546405       -9.835590   \n",
       "2          0       8.0  143.298294       -1.706207       -9.270792   \n",
       "3          0       8.0  143.298294       -1.416705       -9.548032   \n",
       "4          0       8.0  143.298294       -0.598145       -9.853534   \n",
       "\n",
       "   acceleration_z    gyro_x    gyro_y    gyro_z  second     Speed  \n",
       "0       -2.014999 -0.016245 -0.094040  0.070732     0.0  3.442991  \n",
       "1       -2.038925 -0.047092 -0.078874  0.043187     1.0  0.228454  \n",
       "2       -1.209448 -0.028965 -0.032652  0.015390     2.0  0.228454  \n",
       "3       -1.860977 -0.022413  0.005049 -0.025753     3.0  0.228454  \n",
       "4       -1.378574 -0.014297 -0.046206  0.021902     4.0  0.228454  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA PROCESSING(FOR TEST SET)\n",
    "#Reset index for test_X, for applying smooth_feature & negative functions later\n",
    "#Note that train_X index has already been reset via merge function\n",
    "test_X.reset_index(drop=True,inplace=True)\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#This is a function used to fix inaccurate values due to GPS inaccuracy\n",
    "#1) For each ride, get dataframe for that ride(df) & dataframe for that ride with rows accuracy > 10(df_filtered)\n",
    "#2) Pass indices of both dataframes to 2 lists(idx & idx_filtered)\n",
    "#3) For each index in idx_filtered(inaccurate values), search for nearest 2 indices that are in idx(accurate values), subjected to idx range\n",
    "#4) Using the index in idx_filtered ,range between nearest 2 indices in idx & range between idx_filtered index & 1st nearest idx, use formula to set new value for inaccurate value \n",
    "#5) If either of nearest 2 indices is not found, entire row containing the inaccurate value is dropped\n",
    "#6) The formula is designed such that inaccurate values will be replaced with accurate values that are gradually increasing/decreasing between nearest 2 accurate values \n",
    "def smooth_features(input_X,bookingID):\n",
    "    df = input_X[input_X.bookingID == bookingID]\n",
    "    df_filtered = input_X[(input_X.bookingID == bookingID) & (input_X.Accuracy > 10)]\n",
    "    idx_filtered = df_filtered.index.values.tolist()\n",
    "    idx = df.index.values.tolist()\n",
    "        \n",
    "    for i in idx_filtered:\n",
    "        increment_start = 1\n",
    "        increment_end = 1\n",
    "        start = i\n",
    "        end = i\n",
    "        while (idx[0] <= start-increment_start < idx[-1]) & (start-increment_start in idx_filtered):\n",
    "            increment_start += 1   \n",
    "        start -= increment_start     \n",
    "        if start >= idx[0]:\n",
    "            while (idx[0] < end+increment_end <= idx[-1]) & (end+increment_end in idx_filtered):\n",
    "                increment_end += 1\n",
    "            end += increment_end    \n",
    "            if end <= idx[-1]:\n",
    "                df.loc[i,'Speed'] = ((df['Speed'][end]-df['Speed'][start])/(end-start))*(i-start)+df['Speed'][start]  \n",
    "                df.loc[i,'acceleration_x'] = ((df['acceleration_x'][end]-df['acceleration_x'][start])/(end-start))*(i-start)+df['acceleration_x'][start] \n",
    "                df.loc[i,'acceleration_y'] = ((df['acceleration_y'][end]-df['acceleration_y'][start])/(end-start))*(i-start)+df['acceleration_y'][start] \n",
    "                df.loc[i,'acceleration_z'] = ((df['acceleration_z'][end]-df['acceleration_z'][start])/(end-start))*(i-start)+df['acceleration_z'][start] \n",
    "                df.loc[i,'gyro_x'] = ((df['gyro_x'][end]-df['gyro_x'][start])/(end-start))*(i-start)+df['gyro_x'][start] \n",
    "                df.loc[i,'gyro_y'] = ((df['gyro_y'][end]-df['gyro_y'][start])/(end-start))*(i-start)+df['gyro_y'][start] \n",
    "                df.loc[i,'gyro_z'] = ((df['gyro_z'][end]-df['gyro_z'][start])/(end-start))*(i-start)+df['gyro_z'][start]            \n",
    "            else:\n",
    "                df.drop([i],inplace=True)\n",
    "        else:\n",
    "            df.drop([i],inplace=True)\n",
    "    return df\n",
    "\n",
    "#Aply smoothing to train set\n",
    "#Create a dataframe for new training set values\n",
    "#For each ride, smooth variables & append to new dataframe\n",
    "#I used parallel processing since it takes a while to finish running\n",
    "train_X_new = pd.DataFrame(columns=['bookingID','Accuracy','Bearing','acceleration_x','acceleration_y','acceleration_z','gyro_x','gyro_y','gyro_z','second','Speed','label'],dtype=np.int64)\n",
    "features = Parallel(n_jobs=-1, verbose=2)(delayed(smooth_features)(train_X,i) for i in train_y.bookingID.unique()) \n",
    "for i, feat in tqdm(enumerate(features)):\n",
    "    train_X_new = train_X_new.append(feat)\n",
    "    \n",
    "del features\n",
    "gc.collect()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 317 tasks      | elapsed:  4.5min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-dce740b2533a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#For each ride, smooth variables & append to new dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_X_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bookingID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Bearing'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'acceleration_x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'acceleration_y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'acceleration_z'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gyro_x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gyro_y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gyro_z'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'second'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Speed'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbookingID\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtest_X_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_X_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Apply smoothing to test set(FOR TEST SET)\n",
    "#Create a dataframe for new test set values \n",
    "#For each ride, smooth variables & append to new dataframe\n",
    "#I used parallel processing since it takes a while to finish running\n",
    "test_X_new = pd.DataFrame(columns=['bookingID','Accuracy','Bearing','acceleration_x','acceleration_y','acceleration_z','gyro_x','gyro_y','gyro_z','second','Speed','label'],dtype=np.int64)\n",
    "features = Parallel(n_jobs=-1, verbose=2)(delayed(smooth_features)(test_X,i) for i in test_X.bookingID.unique()) \n",
    "for i, feat in tqdm(enumerate(features)):\n",
    "    test_X_new = test_X_new.append(feat)\n",
    "    \n",
    "del features\n",
    "gc.collect()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Check number of rides left in new training set\n",
    "train_X_new.bookingID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Check first 5 rows of new training set\n",
    "train_X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Save new training set to csv file, in case kernel is accidently-disconnected\n",
    "train_X_new.to_csv('train_X_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING(FOR TEST SET)\n",
    "#Save new test set to csv file, in case kernel is accidently-disconnected\n",
    "test_X_new.to_csv('test_X_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Reading new training set file\n",
    "train_X_new = pd.read_csv('train_X_new.csv')\n",
    "train_X_new.set_index('Unnamed: 0',inplace=True)\n",
    "train_X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING(FOR TEST SET)\n",
    "#Reading new test set file\n",
    "test_X_new = pd.read_csv('test_X_new.csv')\n",
    "test_X_new.set_index('Unnamed: 0',inplace=True)\n",
    "test_X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#This is a function similar to the above, used to fix negative Speed values \n",
    "#I can't do both together since some negative speed values have low accuracy values\n",
    "#1) For each ride, get dataframe for that ride(df) & dataframe for that ride with rows speed < 0(df_filtered)\n",
    "#2) Pass indices of both dataframes to 2 lists(idx & idx_filtered)\n",
    "#3) For each index in idx_filtered(negative speed values), search for nearest 2 indices that are in idx(positive speed values), subjected to idx range\n",
    "#4) Using the index in idx_filtered ,range between nearest 2 indices in idx & range between idx_filtered index & 1st nearest idx, use formula to set new value for negative speed value \n",
    "#5) If either of nearest 2 indices is not found, entire row containing the negative speed value is dropped\n",
    "#6) The formula is designed such that negative speed values will be replaced with speed values that are gradually increasing/decreasing between nearest 2 positive speed values \n",
    "def negative_features(input_X,bookingID):\n",
    "    df = input_X[input_X.bookingID == bookingID]\n",
    "    df_filtered = input_X[(input_X.bookingID == bookingID) & (input_X.Speed < 0)]\n",
    "    idx_filtered = df_filtered.index.values.tolist()\n",
    "    idx = df.index.values.tolist()\n",
    "        \n",
    "    for i in idx_filtered:\n",
    "        increment_start = 1\n",
    "        increment_end = 1\n",
    "        start = i\n",
    "        end = i\n",
    "        while (idx[0] <= start-increment_start < idx[-1]) & (start-increment_start in idx_filtered):\n",
    "            increment_start += 1   \n",
    "        start -= increment_start     \n",
    "        if start >= idx[0]:\n",
    "            while (idx[0] < end+increment_end <= idx[-1]) & (end+increment_end in idx_filtered):\n",
    "                increment_end += 1\n",
    "            end += increment_end    \n",
    "            if end <= idx[-1]:\n",
    "                df.loc[i,'Speed'] = ((df['Speed'][end]-df['Speed'][start])/(end-start))*(i-start)+df['Speed'][start]  \n",
    "                df.loc[i,'acceleration_x'] = ((df['acceleration_x'][end]-df['acceleration_x'][start])/(end-start))*(i-start)+df['acceleration_x'][start] \n",
    "                df.loc[i,'acceleration_y'] = ((df['acceleration_y'][end]-df['acceleration_y'][start])/(end-start))*(i-start)+df['acceleration_y'][start] \n",
    "                df.loc[i,'acceleration_z'] = ((df['acceleration_z'][end]-df['acceleration_z'][start])/(end-start))*(i-start)+df['acceleration_z'][start] \n",
    "                df.loc[i,'gyro_x'] = ((df['gyro_x'][end]-df['gyro_x'][start])/(end-start))*(i-start)+df['gyro_x'][start] \n",
    "                df.loc[i,'gyro_y'] = ((df['gyro_y'][end]-df['gyro_y'][start])/(end-start))*(i-start)+df['gyro_y'][start] \n",
    "                df.loc[i,'gyro_z'] = ((df['gyro_z'][end]-df['gyro_z'][start])/(end-start))*(i-start)+df['gyro_z'][start]            \n",
    "            else:\n",
    "                df.drop([i],inplace=True)\n",
    "        else:\n",
    "            df.drop([i],inplace=True)\n",
    "    return df\n",
    "#Create a dataframe for new training set values(coontaining positive speed values only)\n",
    "#For each ride, smooth speed values & append to new dataframe\n",
    "train_X_final = pd.DataFrame(columns=['bookingID','Accuracy','Bearing','acceleration_x','acceleration_y','acceleration_z','gyro_x','gyro_y','gyro_z','second','Speed','label'])\n",
    "features = Parallel(n_jobs=-1, verbose=2)(delayed(negative_features)(train_X_new,i) for i in train_X_new.bookingID.unique()) \n",
    "for i, feat in tqdm(enumerate(features)):\n",
    "    train_X_final = train_X_final.append(feat)\n",
    "    \n",
    "del features\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING(FOR TEST SET)\n",
    "#Apply negative_features to test set \n",
    "test_X_final = pd.DataFrame(columns=['bookingID','Accuracy','Bearing','acceleration_x','acceleration_y','acceleration_z','gyro_x','gyro_y','gyro_z','second','Speed','label'])\n",
    "features = Parallel(n_jobs=-1, verbose=2)(delayed(negative_features)(test_X_new,i) for i in test_X_new.bookingID.unique()) \n",
    "for i, feat in tqdm(enumerate(features)):\n",
    "    test_X_final = test_X_final.append(feat)\n",
    "    \n",
    "del features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Check first 5 rows of new training set\n",
    "train_X_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Again, save new training set, in case kernel disconnects\n",
    "train_X_final.to_csv('train_X_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING(FOR TEST SET)\n",
    "#Again, save new training set, in case kernel disconnects \n",
    "test_X_final.to_csv('test_X_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookingID</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Bearing</th>\n",
       "      <th>acceleration_x</th>\n",
       "      <th>acceleration_y</th>\n",
       "      <th>acceleration_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>second</th>\n",
       "      <th>Speed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1181233</th>\n",
       "      <td>1.116691e+11</td>\n",
       "      <td>5.736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.881093</td>\n",
       "      <td>10.127783</td>\n",
       "      <td>1.503605</td>\n",
       "      <td>-0.024443</td>\n",
       "      <td>-0.055418</td>\n",
       "      <td>-0.008494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.156902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181234</th>\n",
       "      <td>1.116691e+11</td>\n",
       "      <td>5.961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392661</td>\n",
       "      <td>10.185245</td>\n",
       "      <td>1.680781</td>\n",
       "      <td>0.103839</td>\n",
       "      <td>0.041098</td>\n",
       "      <td>0.049538</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.232407</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181235</th>\n",
       "      <td>1.116691e+11</td>\n",
       "      <td>5.302</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.462095</td>\n",
       "      <td>10.474953</td>\n",
       "      <td>-0.668003</td>\n",
       "      <td>0.042141</td>\n",
       "      <td>0.034990</td>\n",
       "      <td>0.089855</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.318302</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181236</th>\n",
       "      <td>1.116691e+11</td>\n",
       "      <td>4.990</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.026337</td>\n",
       "      <td>9.852441</td>\n",
       "      <td>-1.240234</td>\n",
       "      <td>-0.059873</td>\n",
       "      <td>-0.010825</td>\n",
       "      <td>-0.010327</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.916334</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181237</th>\n",
       "      <td>1.116691e+11</td>\n",
       "      <td>6.000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.390267</td>\n",
       "      <td>9.763853</td>\n",
       "      <td>-0.385478</td>\n",
       "      <td>-0.008561</td>\n",
       "      <td>-0.005327</td>\n",
       "      <td>-0.010327</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.250799</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               bookingID  Accuracy  Bearing  acceleration_x  acceleration_y  \\\n",
       "Unnamed: 0                                                                    \n",
       "1181233     1.116691e+11     5.736      0.0        0.881093       10.127783   \n",
       "1181234     1.116691e+11     5.961      0.0        0.392661       10.185245   \n",
       "1181235     1.116691e+11     5.302      2.0       -0.462095       10.474953   \n",
       "1181236     1.116691e+11     4.990      2.0       -0.026337        9.852441   \n",
       "1181237     1.116691e+11     6.000      3.0       -0.390267        9.763853   \n",
       "\n",
       "            acceleration_z    gyro_x    gyro_y    gyro_z  second     Speed  \\\n",
       "Unnamed: 0                                                                   \n",
       "1181233           1.503605 -0.024443 -0.055418 -0.008494     0.0  3.156902   \n",
       "1181234           1.680781  0.103839  0.041098  0.049538     1.0  3.232407   \n",
       "1181235          -0.668003  0.042141  0.034990  0.089855     2.0  3.318302   \n",
       "1181236          -1.240234 -0.059873 -0.010825 -0.010327     3.0  2.916334   \n",
       "1181237          -0.385478 -0.008561 -0.005327 -0.010327     4.0  4.250799   \n",
       "\n",
       "            label  \n",
       "Unnamed: 0         \n",
       "1181233       0.0  \n",
       "1181234       0.0  \n",
       "1181235       0.0  \n",
       "1181236       0.0  \n",
       "1181237       0.0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA PROCESSING\n",
    "#Read new training set file\n",
    "train_X_final = pd.read_csv('train_X_final.csv')\n",
    "train_X_final.set_index('Unnamed: 0',inplace=True)\n",
    "train_X_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING(FOR TEST SET)\n",
    "#Read new test set file \n",
    "test_X_final = pd.read_csv('test_X_final.csv')\n",
    "test_X_final.set_index('Unnamed: 0',inplace=True)\n",
    "test_X_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Check minimum & maximum ride durations\n",
    "minimum = 1000\n",
    "maximum = 0\n",
    "for i in tqdm(train_X_final.bookingID.unique()):\n",
    "    if len(train_X_final[train_X_final['bookingID'] == i]) < minimum:\n",
    "        minimum = len(train_X_final[train_X_final['bookingID'] == i])\n",
    "    elif len(train_X_final[train_X_final['bookingID'] == i]) > maximum:\n",
    "        maximum = len(train_X_final[train_X_final['bookingID'] == i])\n",
    "minimum,maximum        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b759d5547a4095a4ab30e85bcc95ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19826), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#DATA PROCESSING\n",
    "#Find rides lasting less than 15 seconds & add their bookingID to to_drop\n",
    "#Of course, feel free to play with different thresholds\n",
    "#Due to time constraint, I decided to stick with 15 seconds\n",
    "to_drop = []\n",
    "for i in tqdm(train_X_final.bookingID.unique()):\n",
    "    if len(train_X_final[train_X_final.bookingID == i]) < 15:\n",
    "        to_drop.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Drop rides less than 15 seconds, whose bookingIDs are in to_drop\n",
    "train_X_final = train_X_final[~train_X_final['bookingID'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING(FOR TEST SET) \n",
    "#Find rides lasting less than 15 seconds & add their bookingID to to_drop \n",
    "to_drop = []\n",
    "for i in tqdm(test_X_final.bookingID.unique()):\n",
    "    if len(test_X_final[test_X_final.bookingID == i]) < 15:\n",
    "        to_drop.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING(FOR TEST SET)\n",
    "#Drop rides less than 15 seconds, whose bookingIDs are in to_drop \n",
    "test_X_final = test_X_final[~test_X_final['bookingID'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Similarly for label set, drop rides that are not in new training set\n",
    "train_y = train_y[train_y.bookingID.isin(train_X_final.bookingID.unique())]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19788"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA PROCESSING\n",
    "#Final look at number of rides remaining in training set\n",
    "train_X_final.bookingID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING(FOR TEST SET)\n",
    "#Final look at number of rides remaining in test set \n",
    "test_X_final.bookingID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PROCESSING\n",
    "#Sanity check to ensure number of unique rides in both label set & training set are the same\n",
    "assert len(train_y) == train_X_final.bookingID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING\n",
    "#Create a function to calculate rate of change for creating rate-related features\n",
    "def calc_change_rate(x):\n",
    "    change = (np.diff(x) / x[:-1])\n",
    "    #change = change[np.nonzero(change)[0]]\n",
    "    change = change[~np.isnan(change)]\n",
    "    change = change[change != -np.inf]\n",
    "    change = change[change != np.inf]\n",
    "    return change.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING\n",
    "#seg is the data segment for each unique ride\n",
    "#seg_id is the bookingID for each unique ride\n",
    "#For each segment, calculate summary stats & store it in X\n",
    "def create_features(seg_id,seg,X):\n",
    "    xs = pd.Series(seg['Speed'].values)    \n",
    "    X.loc[seg_id, 'mean_Speed'] = np.mean(xs)\n",
    "    rolling_speed_2_std = xs.rolling(2).std()\n",
    "    X.loc[seg_id, 'rolling_speed_2_std'] = rolling_speed_2_std.mean()\n",
    "    speed_diff = np.diff(xs)\n",
    "    X.loc[seg_id, 'hard_braking_speed_min'] = speed_diff[speed_diff < 0].min() if len(speed_diff[speed_diff < 0]) > 0 else 0\n",
    "    X.loc[seg_id, 'hard_braking_speed_std'] = speed_diff[speed_diff < 0].std() if len(speed_diff[speed_diff < 0]) > 0 else 0\n",
    "    X.loc[seg_id, 'hard_braking_speed_ptp'] = np.ptp(speed_diff[speed_diff < 0]) if len(speed_diff[speed_diff < 0]) > 0 else 0\n",
    "    X.loc[seg_id, 'hard_braking_speed_num_peaks_10'] = feature_calculators.number_peaks(speed_diff[speed_diff < 0],10) if len(speed_diff[speed_diff < 0]) > 0 else 0\n",
    "    X.loc[seg_id, 'hard_braking_speed_be_5'] = feature_calculators.binned_entropy(speed_diff[speed_diff < 0],5) if len(speed_diff[speed_diff < 0]) > 0 else 0\n",
    "    \n",
    "    X.loc[seg_id, 'hard_acc_speed_max'] = speed_diff[speed_diff > 0].max() if len(speed_diff[speed_diff > 0]) > 0 else 0\n",
    "    X.loc[seg_id, 'hard_acc_speed_std'] = speed_diff[speed_diff > 0].std() if len(speed_diff[speed_diff > 0]) > 0 else 0\n",
    "    X.loc[seg_id, 'hard_acc_speed_ptp'] = np.ptp(speed_diff[speed_diff > 0]) if len(speed_diff[speed_diff > 0]) > 0 else 0\n",
    "    X.loc[seg_id, 'hard_acc_speed_num_peaks_10'] = feature_calculators.number_peaks(speed_diff[speed_diff > 0],10) if len(speed_diff[speed_diff > 0]) > 0 else 0\n",
    "    X.loc[seg_id, 'hard_acc_speed_be_5'] = feature_calculators.binned_entropy(speed_diff[speed_diff < 0],5) if len(speed_diff[speed_diff < 0]) > 0 else 0\n",
    "    rolling_pos_speed_2_std = xs[xs>0].rolling(2).std()\n",
    "    X.loc[seg_id, 'rolling_pos_speed_2_std'] = rolling_pos_speed_2_std.mean()\n",
    "    \n",
    "    xz = pd.Series(seg['acceleration_z'].values)\n",
    "    rolling_acc_z_2_diff = xz.rolling(2).apply(lambda x:np.diff(x))\n",
    "    X.loc[seg_id, 'rolling_acc_z_diff_mean'] = rolling_acc_z_2_diff.mean()\n",
    "    X.loc[seg_id, 'max_acc_z'] = xz.max()\n",
    "    X.loc[seg_id, 'min_acc_z'] = xz.min()\n",
    "    X.loc[seg_id, 'med_acc_z'] = np.median(xz)\n",
    "    X.loc[seg_id, 'p90_acc_z'] = np.percentile(xz,90)\n",
    "    X.loc[seg_id, 'mean_acc_z_pos'] = xz[xz>0].mean()\n",
    "    X.loc[seg_id, 'mean_acc_z_neg'] = xz[xz<0].mean()\n",
    "    X.loc[seg_id, 'num_peaks_pos_acc_z_10'] = feature_calculators.number_peaks(xz[xz > 0],10) if len(xz[xz > 0]) > 0 else 0\n",
    "    X.loc[seg_id, 'num_peaks_neg_acc_z_10'] = feature_calculators.number_peaks(xz[xz < 0],10) if len(xz[xz < 0]) > 0 else 0\n",
    "    \n",
    "    xy = pd.Series(seg['acceleration_y'].values)\n",
    "    rolling_acc_y_2_diff = xy.rolling(2).apply(lambda x:np.diff(x))\n",
    "    X.loc[seg_id, 'rolling_acc_y_diff_mean'] = rolling_acc_y_2_diff.mean()\n",
    "    X.loc[seg_id, 'max_acc_y'] = xy.max()\n",
    "    X.loc[seg_id, 'min_acc_y'] = xy.min()\n",
    "    X.loc[seg_id, 'med_acc_y'] = np.median(xy)\n",
    "    X.loc[seg_id, 'p90_acc_y'] = np.percentile(xy,90)\n",
    "    X.loc[seg_id, 'mean_acc_y_pos'] = xy[xy>0].mean()\n",
    "    X.loc[seg_id, 'mean_acc_y_neg'] = xy[xy<0].mean()\n",
    "    X.loc[seg_id, 'num_peaks_pos_acc_y_10'] = feature_calculators.number_peaks(xy[xy > 0],10) if len(xy[xy > 0]) > 0 else 0\n",
    "    X.loc[seg_id, 'num_peaks_neg_acc_y_10'] = feature_calculators.number_peaks(xy[xy < 0],10) if len(xy[xy < 0]) > 0 else 0\n",
    "    \n",
    "    xx = pd.Series(seg['acceleration_x'].values)\n",
    "    rolling_acc_x_2_diff = xx.rolling(2).apply(lambda x:np.diff(x))\n",
    "    X.loc[seg_id, 'rolling_acc_x_diff_mean'] = rolling_acc_x_2_diff.mean()\n",
    "    X.loc[seg_id, 'max_acc_x'] = xx.max()\n",
    "    X.loc[seg_id, 'min_acc_x'] = xx.min()\n",
    "    X.loc[seg_id, 'med_acc_x'] = np.median(xx)\n",
    "    X.loc[seg_id, 'p90_acc_x'] = np.percentile(xx,90)\n",
    "    X.loc[seg_id, 'mean_acc_x_pos'] = xx[xx>0].mean()\n",
    "    X.loc[seg_id, 'mean_acc_x_neg'] = xx[xx<0].mean()\n",
    "    X.loc[seg_id, 'num_peaks_pos_acc_x_10'] = feature_calculators.number_peaks(xx[xx > 0],10) if len(xx[xx > 0]) > 0 else 0\n",
    "    X.loc[seg_id, 'num_peaks_neg_acc_x_10'] = feature_calculators.number_peaks(xx[xx < 0],10) if len(xx[xx < 0]) > 0 else 0\n",
    "    \n",
    "    X.loc[seg_id, 'trip_time'] = len(seg)\n",
    "     \n",
    "    ##hardbrake combos\n",
    "    temp = seg[(seg['gyro_x'] > 0) &(seg['acceleration_y'] < 0)] \n",
    "    gx_temp = pd.Series(temp['gyro_x'].values)\n",
    "    ay_temp = pd.Series(temp['acceleration_y'].values)                                 \n",
    "    s_temp = pd.Series(temp['Speed'].values)\n",
    "    X.loc[seg_id, 'gx_ay_hardbrake_min'] = np.multiply(gx_temp,ay_temp).min()\n",
    "    X.loc[seg_id, 'gx_ay_hardbrake_std'] = np.multiply(gx_temp,ay_temp).std()\n",
    "    X.loc[seg_id, 'gx_ay_hardbrake_mean'] = np.multiply(gx_temp,ay_temp).mean()\n",
    "    X.loc[seg_id, 'gx_ay_hardbrake_diff_mean'] = np.diff(np.multiply(gx_temp,ay_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'gx_s_hardbrake_max'] = np.multiply(gx_temp,s_temp).max()\n",
    "    X.loc[seg_id, 'gx_s_hardbrake_std'] = np.multiply(gx_temp,s_temp).std()\n",
    "    X.loc[seg_id, 'gx_s_hardbrake_mean'] = np.multiply(gx_temp,s_temp).mean()\n",
    "    #to be del\n",
    "    X.loc[seg_id, 'gx_s_hardbrake_diff_mean'] = np.diff(np.multiply(gx_temp,s_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'ay_s_hardbrake_min'] = np.multiply(ay_temp,s_temp).min()\n",
    "    X.loc[seg_id, 'ay_s_hardbrake_std'] = np.multiply(ay_temp,s_temp).std()\n",
    "    X.loc[seg_id, 'ay_s_hardbrake_mean'] = np.multiply(ay_temp,s_temp).mean()\n",
    "    X.loc[seg_id, 'ay_s_hardbrake_diff_mean'] = np.diff(np.multiply(ay_temp,s_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'gx_hardbrake_max'] = gx_temp.max()\n",
    "    X.loc[seg_id, 'gx_hardbrake_mean'] = gx_temp.mean()\n",
    "    X.loc[seg_id, 'gx_hardbrake_med'] = np.median(gx_temp)\n",
    "    X.loc[seg_id, 'gx_hardbrake_std'] = gx_temp.std()\n",
    "    X.loc[seg_id, 'gx_hardbrake_90p'] = np.percentile(gx_temp,90) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 'gx_hardbrake_diff_mean'] = np.diff(gx_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'ay_hardbrake_min'] = ay_temp.min()\n",
    "    X.loc[seg_id, 'ay_hardbrake_mean'] = ay_temp.mean()\n",
    "    X.loc[seg_id, 'ay_hardbrake_med'] = np.median(ay_temp)\n",
    "    #X.loc[seg_id, 'ay_hardbrake_std'] = ay_temp.std()\n",
    "    X.loc[seg_id, 'ay_hardbrake_10p'] = np.percentile(ay_temp,10) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 'ay_hardbrake_diff_mean'] = np.diff(ay_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 's_hardbrake_min'] = s_temp.min()\n",
    "    X.loc[seg_id, 's_hardbrake_mean'] = s_temp.mean()\n",
    "    X.loc[seg_id, 's_hardbrake_med'] = np.median(s_temp)\n",
    "    X.loc[seg_id, 's_hardbrake_std'] = s_temp.std()\n",
    "    #X.loc[seg_id, 's_hardbrake_10p'] = np.percentile(s_temp,10) if len(temp) > 0 else 0\n",
    "    #X.loc[seg_id, 's_hardbrake_diff_mean'] = np.diff(s_temp).mean()\n",
    "    ##hardbrake combos\n",
    "    \n",
    "    ##hardacc combos\n",
    "    temp = seg[(seg['gyro_x'] > 0) &(seg['acceleration_y'] > 0)] \n",
    "    gx_temp = pd.Series(temp['gyro_x'].values)\n",
    "    ay_temp = pd.Series(temp['acceleration_y'].values)\n",
    "    s_temp = pd.Series(temp['Speed'].values)\n",
    "    X.loc[seg_id, 'gx_ay_hardacc_max'] = np.multiply(gx_temp,ay_temp).max()\n",
    "    X.loc[seg_id, 'gx_ay_hardacc_std'] = np.multiply(gx_temp,ay_temp).std()\n",
    "    X.loc[seg_id, 'gx_ay_hardacc_mean'] = np.multiply(gx_temp,ay_temp).mean()\n",
    "    X.loc[seg_id, 'gx_ay_hardacc_diff_mean'] = np.diff(np.multiply(gx_temp,ay_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'gx_s_hardacc_max'] = np.multiply(gx_temp,s_temp).max()\n",
    "    X.loc[seg_id, 'gx_s_hardacc_std'] = np.multiply(gx_temp,s_temp).std()\n",
    "    X.loc[seg_id, 'gx_s_hardacc_mean'] = np.multiply(gx_temp,s_temp).mean()\n",
    "    X.loc[seg_id, 'gx_s_hardacc_diff_mean'] = np.diff(np.multiply(gx_temp,s_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'ay_s_hardacc_max'] = np.multiply(ay_temp,s_temp).max()\n",
    "    X.loc[seg_id, 'ay_s_hardacc_std'] = np.multiply(ay_temp,s_temp).std()\n",
    "    X.loc[seg_id, 'ay_s_hardacc_mean'] = np.multiply(ay_temp,s_temp).mean()\n",
    "    X.loc[seg_id, 'ay_s_hardacc_diff_mean'] = np.diff(np.multiply(ay_temp,s_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'gx_hardacc_max'] = gx_temp.max()\n",
    "    X.loc[seg_id, 'gx_hardacc_mean'] = gx_temp.mean()\n",
    "    X.loc[seg_id, 'gx_hardacc_med'] = np.median(gx_temp)\n",
    "    X.loc[seg_id, 'gx_hardacc_std'] = gx_temp.std()\n",
    "    X.loc[seg_id, 'gx_hardacc_90p'] = np.percentile(gx_temp,90) if len(temp) > 1 else 0\n",
    "    X.loc[seg_id, 'gx_hardacc_diff_mean'] = np.diff(gx_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'ay_hardacc_max'] = ay_temp.max()\n",
    "    X.loc[seg_id, 'ay_hardacc_mean'] = ay_temp.mean()\n",
    "    X.loc[seg_id, 'ay_hardacc_med'] = np.median(ay_temp)\n",
    "    X.loc[seg_id, 'ay_hardacc_std'] = ay_temp.std()\n",
    "    X.loc[seg_id, 'ay_hardacc_90p'] = np.percentile(ay_temp,90) if len(temp) > 1 else 0\n",
    "    X.loc[seg_id, 'ay_hardacc_diff_mean'] = np.diff(ay_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 's_hardacc_max'] = s_temp.max()\n",
    "    X.loc[seg_id, 's_hardacc_mean'] = s_temp.mean()\n",
    "    X.loc[seg_id, 's_hardacc_med'] = np.median(s_temp)\n",
    "    X.loc[seg_id, 's_hardacc_std'] = s_temp.std()\n",
    "    X.loc[seg_id, 's_hardacc_90p'] = np.percentile(s_temp,90) if len(temp) > 1 else 0\n",
    "    X.loc[seg_id, 's_hardacc_diff_mean'] = np.diff(s_temp).mean()\n",
    "    ##hardacc combos\n",
    "    \n",
    "    ##hardright combos\n",
    "    temp = seg[(seg['gyro_z'] < 0) &(seg['acceleration_x'] > 0)]\n",
    "    ax_temp = pd.Series(temp['acceleration_x'].values)\n",
    "    gz_temp = pd.Series(temp['gyro_z'].values)\n",
    "    s_temp = pd.Series(temp['Speed'].values)\n",
    "    X.loc[seg_id, 'ax_gz_hardright_min'] = np.multiply(ax_temp,gz_temp).min()\n",
    "    X.loc[seg_id, 'ax_gz_hardright_std'] = np.multiply(ax_temp,gz_temp).std()\n",
    "    X.loc[seg_id, 'ax_gz_hardright_mean'] = np.multiply(ax_temp,gz_temp).mean()\n",
    "    X.loc[seg_id, 'ax_gz_hardright_diff_mean'] = np.diff(np.multiply(ax_temp,gz_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 's_gz_hardright_min'] = np.multiply(s_temp,gz_temp).min()\n",
    "    X.loc[seg_id, 's_gz_hardright_std'] = np.multiply(s_temp,gz_temp).std()\n",
    "    X.loc[seg_id, 's_gz_hardright_mean'] = np.multiply(s_temp,gz_temp).mean()\n",
    "    X.loc[seg_id, 's_gz_hardright_diff_mean'] = np.diff(np.multiply(s_temp,gz_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'ax_s_hardright_max'] = np.multiply(ax_temp,s_temp).max()\n",
    "    X.loc[seg_id, 'ax_s_hardright_std'] = np.multiply(ax_temp,s_temp).std()\n",
    "    X.loc[seg_id, 'ax_s_hardright_mean'] = np.multiply(ax_temp,s_temp).mean()\n",
    "    X.loc[seg_id, 'ax_s_hardright_diff_mean'] = np.diff(np.multiply(ax_temp,s_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'gz_hardright_min'] = gz_temp.min()\n",
    "    X.loc[seg_id, 'gz_hardright_mean'] = gz_temp.mean()\n",
    "    X.loc[seg_id, 'gz_hardright_med'] = np.median(gz_temp)\n",
    "    X.loc[seg_id, 'gz_hardright_std'] = gz_temp.std()\n",
    "    X.loc[seg_id, 'gz_hardright_10p'] = np.percentile(gz_temp,10) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 'gz_hardright_diff_mean'] = np.diff(gz_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'ax_hardright_max'] = ax_temp.max()\n",
    "    X.loc[seg_id, 'ax_hardright_mean'] = ax_temp.mean()\n",
    "    X.loc[seg_id, 'ax_hardright_med'] = np.median(ax_temp)\n",
    "    X.loc[seg_id, 'ax_hardright_std'] = ax_temp.std()\n",
    "    X.loc[seg_id, 'ax_hardright_90p'] = np.percentile(ax_temp,90) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 'ax_hardright_diff_mean'] = np.diff(ax_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 's_hardright_max'] = s_temp.max()\n",
    "    X.loc[seg_id, 's_hardright_mean'] = s_temp.mean()\n",
    "    X.loc[seg_id, 's_hardright_med'] = np.median(s_temp)\n",
    "    X.loc[seg_id, 's_hardright_std'] = s_temp.std()\n",
    "    X.loc[seg_id, 's_hardright_90p'] = np.percentile(s_temp,90) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 's_hardright_diff_mean'] = np.diff(s_temp).mean()\n",
    "    ##hardright combos\n",
    "    \n",
    "    ##hardleft combos\n",
    "    temp = seg[(seg['gyro_z'] > 0) &(seg['acceleration_x'] < 0)]\n",
    "    ax_temp = pd.Series(temp['acceleration_x'].values)\n",
    "    gz_temp = pd.Series(temp['gyro_z'].values)\n",
    "    s_temp = pd.Series(temp['Speed'].values)\n",
    "    X.loc[seg_id, 'ax_gz_hardleft_min'] = np.multiply(ax_temp,gz_temp).min()\n",
    "    X.loc[seg_id, 'ax_gz_hardleft_std'] = np.multiply(ax_temp,gz_temp).std()\n",
    "    X.loc[seg_id, 'ax_gz_hardleft_mean'] = np.multiply(ax_temp,gz_temp).mean()\n",
    "    X.loc[seg_id, 'ax_gz_hardleft_diff_mean'] = np.diff(np.multiply(ax_temp,gz_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 's_gz_hardleft_max'] = np.multiply(s_temp,gz_temp).max()\n",
    "    X.loc[seg_id, 's_gz_hardleft_std'] = np.multiply(s_temp,gz_temp).std()\n",
    "    X.loc[seg_id, 's_gz_hardleft_mean'] = np.multiply(s_temp,gz_temp).mean()\n",
    "    X.loc[seg_id, 's_gz_hardleft_diff_mean'] = np.diff(np.multiply(s_temp,gz_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'ax_s_hardleft_min'] = np.multiply(ax_temp,s_temp).min()\n",
    "    X.loc[seg_id, 'ax_s_hardleft_std'] = np.multiply(ax_temp,s_temp).std()\n",
    "    X.loc[seg_id, 'ax_s_hardleft_mean'] = np.multiply(ax_temp,s_temp).mean()\n",
    "    X.loc[seg_id, 'ax_s_hardleft_diff_mean'] = np.diff(np.multiply(ax_temp,s_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'gz_hardleft_max'] = gz_temp.max()\n",
    "    X.loc[seg_id, 'gz_hardleft_mean'] = gz_temp.mean()\n",
    "    X.loc[seg_id, 'gz_hardleft_med'] = np.median(gz_temp)\n",
    "    X.loc[seg_id, 'gz_hardleft_std'] = gz_temp.std()\n",
    "    X.loc[seg_id, 'gz_hardleft_90p'] = np.percentile(gz_temp,90) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 'gz_hardleft_diff_mean'] = np.diff(gz_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'ax_hardleft_min'] = ax_temp.min()\n",
    "    X.loc[seg_id, 'ax_hardleft_mean'] = ax_temp.mean()\n",
    "    X.loc[seg_id, 'ax_hardleft_med'] = np.median(ax_temp)\n",
    "    X.loc[seg_id, 'ax_hardleft_std'] = ax_temp.std()\n",
    "    X.loc[seg_id, 'ax_hardleft_10p'] = np.percentile(ax_temp,10) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 'ax_hardleft_diff_mean'] = np.diff(ax_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 's_hardleft_max'] = s_temp.max()\n",
    "    X.loc[seg_id, 's_hardleft_mean'] = s_temp.mean()\n",
    "    X.loc[seg_id, 's_hardleft_med'] = np.median(s_temp)\n",
    "    X.loc[seg_id, 's_hardleft_std'] = s_temp.std()\n",
    "    X.loc[seg_id, 's_hardleft_90p'] = np.percentile(s_temp,90) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 's_hardleft_diff_mean'] = np.diff(s_temp).mean()\n",
    "    ##hardleft combos\n",
    "    \n",
    "    ##hardswerveright combos\n",
    "    temp = seg[(seg['gyro_y'] > 0) &(seg['acceleration_x'] > 0)] \n",
    "    ax_temp = pd.Series(temp['acceleration_x'].values)\n",
    "    gy_temp = pd.Series(temp['gyro_y'].values)\n",
    "    s_temp = pd.Series(temp['Speed'].values)\n",
    "    X.loc[seg_id, 'gy_ax_hardSright_max'] = np.multiply(gy_temp,ax_temp).max()\n",
    "    X.loc[seg_id, 'gy_ax_hardSright_std'] = np.multiply(gy_temp,ax_temp).std()\n",
    "    X.loc[seg_id, 'gy_ax_hardSright_mean'] = np.multiply(gy_temp,ax_temp).mean()\n",
    "    X.loc[seg_id, 'gy_ax_hardSright_diff_mean'] = np.diff(np.multiply(gy_temp,ax_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'gy_s_hardSright_max'] = np.multiply(gy_temp,s_temp).max()\n",
    "    X.loc[seg_id, 'gy_s_hardSright_std'] = np.multiply(gy_temp,s_temp).std()\n",
    "    X.loc[seg_id, 'gy_s_hardSright_mean'] = np.multiply(gy_temp,s_temp).mean()\n",
    "    X.loc[seg_id, 'gy_s_hardSright_diff_mean'] = np.diff(np.multiply(gy_temp,s_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'ax_s_hardSright_max'] = np.multiply(ax_temp,s_temp).max()\n",
    "    X.loc[seg_id, 'ax_s_hardSright_std'] = np.multiply(ax_temp,s_temp).std()\n",
    "    X.loc[seg_id, 'ax_s_hardSright_mean'] = np.multiply(ax_temp,s_temp).mean()\n",
    "    X.loc[seg_id, 'ax_s_hardSright_diff_mean'] = np.diff(np.multiply(ax_temp,s_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'gy_hardSright_max'] = gy_temp.max()\n",
    "    X.loc[seg_id, 'gy_hardSright_mean'] = gy_temp.mean()\n",
    "    X.loc[seg_id, 'gy_hardSright_med'] = np.median(gy_temp)\n",
    "    X.loc[seg_id, 'gy_hardSright_std'] = gy_temp.std()\n",
    "    X.loc[seg_id, 'gy_hardSright_90p'] = np.percentile(gy_temp,90) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 'gy_hardSright_diff_mean'] = np.diff(gy_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'ax_hardSright_max'] = ax_temp.max()\n",
    "    X.loc[seg_id, 'ax_hardSright_mean'] = ax_temp.mean()\n",
    "    X.loc[seg_id, 'ax_hardSright_med'] = np.median(ax_temp)\n",
    "    X.loc[seg_id, 'ax_hardSright_std'] = ax_temp.std()\n",
    "    X.loc[seg_id, 'ax_hardSright_90p'] = np.percentile(ax_temp,90) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 'ax_hardSright_diff_mean'] = np.diff(ax_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 's_hardSright_max'] = s_temp.max()\n",
    "    X.loc[seg_id, 's_hardSright_mean'] = s_temp.mean()\n",
    "    X.loc[seg_id, 's_hardSright_med'] = np.median(s_temp)\n",
    "    X.loc[seg_id, 's_hardSright_std'] = s_temp.std()\n",
    "    X.loc[seg_id, 's_hardSright_90p'] = np.percentile(s_temp,90) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 's_hardSright_diff_mean'] = np.diff(s_temp).mean()\n",
    "    ##hardswerveright combos\n",
    "    \n",
    "    ##hardswerveleft combos\n",
    "    temp = seg[(seg['gyro_y'] < 0) &(seg['acceleration_x'] < 0)] \n",
    "    ax_temp = pd.Series(temp['acceleration_x'].values)\n",
    "    gy_temp = pd.Series(temp['gyro_y'].values)\n",
    "    s_temp = pd.Series(temp['Speed'].values)\n",
    "    X.loc[seg_id, 'gy_ax_hardSleft_max'] = np.multiply(gy_temp,ax_temp).max()\n",
    "    X.loc[seg_id, 'gy_ax_hardSleft_std'] = np.multiply(gy_temp,ax_temp).std()\n",
    "    X.loc[seg_id, 'gy_ax_hardSleft_mean'] = np.multiply(gy_temp,ax_temp).mean()\n",
    "    X.loc[seg_id, 'gy_ax_hardSleft_diff_mean'] = np.diff(np.multiply(gy_temp,ax_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'ax_s_hardSleft_min'] = np.multiply(ax_temp,s_temp).min()\n",
    "    X.loc[seg_id, 'ax_s_hardSleft_std'] = np.multiply(ax_temp,s_temp).std()\n",
    "    X.loc[seg_id, 'ax_s_hardSleft_mean'] = np.multiply(ax_temp,s_temp).mean()\n",
    "    X.loc[seg_id, 'ax_s_hardSleft_diff_mean'] = np.diff(np.multiply(ax_temp,s_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'gy_s_hardSleft_min'] = np.multiply(gy_temp,s_temp).min()\n",
    "    X.loc[seg_id, 'gy_s_hardSleft_std'] = np.multiply(gy_temp,s_temp).std()\n",
    "    X.loc[seg_id, 'gy_s_hardSleft_mean'] = np.multiply(gy_temp,s_temp).mean()\n",
    "    X.loc[seg_id, 'gy_s_hardSleft_diff_mean'] = np.diff(np.multiply(gy_temp,s_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'gy_hardSleft_min'] = gy_temp.min()\n",
    "    X.loc[seg_id, 'gy_hardSleft_mean'] = gy_temp.mean()\n",
    "    X.loc[seg_id, 'gy_hardSleft_med'] = np.median(gy_temp)\n",
    "    X.loc[seg_id, 'gy_hardSleft_std'] = gy_temp.std()\n",
    "    X.loc[seg_id, 'gy_hardSleft_10p'] = np.percentile(gy_temp,10) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 'gy_hardSleft_diff_mean'] = np.diff(gy_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'ax_hardSleft_min'] = ax_temp.min()\n",
    "    X.loc[seg_id, 'ax_hardSleft_mean'] = ax_temp.mean()\n",
    "    X.loc[seg_id, 'ax_hardSleft_med'] = np.median(ax_temp)\n",
    "    X.loc[seg_id, 'ax_hardSleft_std'] = ax_temp.std()\n",
    "    X.loc[seg_id, 'ax_hardSleft_10p'] = np.percentile(ax_temp,10) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 'ax_hardSleft_diff_mean'] = np.diff(ax_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 's_hardSleft_max'] = s_temp.max()\n",
    "    X.loc[seg_id, 's_hardSleft_mean'] = s_temp.mean()\n",
    "    X.loc[seg_id, 's_hardSleft_med'] = np.median(s_temp)\n",
    "    X.loc[seg_id, 's_hardSleft_std'] = s_temp.std()\n",
    "    X.loc[seg_id, 's_hardSleft_90p'] = np.percentile(s_temp,90) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 's_hardSleft_diff_mean'] = np.diff(s_temp).mean()\n",
    "    ##hardswerveleft combos\n",
    "    \n",
    "    ##hardbump combos\n",
    "    temp = seg[(seg['gyro_x'] > 0) &(seg['acceleration_z'] > 0)] \n",
    "    az_temp = pd.Series(temp['acceleration_z'].values)\n",
    "    gx_temp = pd.Series(temp['gyro_x'].values)\n",
    "    s_temp = pd.Series(temp['Speed'].values)\n",
    "    X.loc[seg_id, 'az_s_hardbump_max'] = np.multiply(az_temp,s_temp).max()\n",
    "    X.loc[seg_id, 'az_s_hardbump_std'] = np.multiply(az_temp,s_temp).std()\n",
    "    X.loc[seg_id, 'az_s_hardbump_mean'] = np.multiply(az_temp,s_temp).mean()\n",
    "    X.loc[seg_id, 'az_s_hardbump_diff_mean'] = np.diff(np.multiply(az_temp,s_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'az_gx_hardbump_max'] = np.multiply(az_temp,gx_temp).max()\n",
    "    X.loc[seg_id, 'az_gx_hardbump_std'] = np.multiply(az_temp,gx_temp).std()\n",
    "    X.loc[seg_id, 'az_gx_hardbump_mean'] = np.multiply(az_temp,gx_temp).mean()\n",
    "    X.loc[seg_id, 'az_gx_hardbump_diff_mean'] = np.diff(np.multiply(az_temp,gx_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 's_gx_hardbump_max'] = np.multiply(s_temp,gx_temp).max()\n",
    "    X.loc[seg_id, 's_gx_hardbump_std'] = np.multiply(s_temp,gx_temp).std()\n",
    "    X.loc[seg_id, 's_gx_hardbump_mean'] = np.multiply(s_temp,gx_temp).mean()\n",
    "    X.loc[seg_id, 's_gx_hardbump_diff_mean'] = np.diff(np.multiply(s_temp,gx_temp)).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'gx_hardbump_max'] = gx_temp.max()\n",
    "    X.loc[seg_id, 'gx_hardbump_mean'] = gx_temp.mean()\n",
    "    X.loc[seg_id, 'gx_hardbump_med'] = np.median(gx_temp)\n",
    "    X.loc[seg_id, 'gx_hardbump_std'] = gx_temp.std()\n",
    "    X.loc[seg_id, 'gx_hardbump_90p'] = np.percentile(gx_temp,90) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 'gx_hardbump_diff_mean'] = np.diff(gx_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'az_hardbump_max'] = az_temp.max()\n",
    "    X.loc[seg_id, 'az_hardbump_mean'] = az_temp.mean()\n",
    "    X.loc[seg_id, 'az_hardbump_med'] = np.median(az_temp)\n",
    "    X.loc[seg_id, 'az_hardbump_std'] = az_temp.std()\n",
    "    X.loc[seg_id, 'az_hardbump_90p'] = np.percentile(az_temp,90) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 'az_hardbump_diff_mean'] = np.diff(az_temp).mean()\n",
    "    \n",
    "    X.loc[seg_id, 's_hardbump_max'] = s_temp.max()\n",
    "    X.loc[seg_id, 's_hardbump_mean'] = s_temp.mean()\n",
    "    X.loc[seg_id, 's_hardbump_med'] = np.median(s_temp)\n",
    "    X.loc[seg_id, 's_hardbump_std'] = s_temp.std()\n",
    "    X.loc[seg_id, 's_hardbump_90p'] = np.percentile(s_temp,90) if len(temp) > 0 else 0\n",
    "    X.loc[seg_id, 's_hardbump_diff_mean'] = np.diff(s_temp).mean()\n",
    "    ##hardbump combos\n",
    "    \n",
    "    #specials\n",
    "    power = np.multiply(xs,xy)\n",
    "    X.loc[seg_id, 'power_mean'] = power.mean()\n",
    "    X.loc[seg_id, 'power_std'] = power.std()\n",
    "    X.loc[seg_id, 'power_max'] = power.max()\n",
    "    X.loc[seg_id, 'power_min'] = power.min()\n",
    "    X.loc[seg_id, 'power_med'] = np.median(power)\n",
    "    X.loc[seg_id, 'power_pos_mean'] = power[power>0].mean()\n",
    "    X.loc[seg_id, 'power_pos_max'] = power[power>0].max()\n",
    "    X.loc[seg_id, 'power_pos_std'] = power[power>0].std()\n",
    "    X.loc[seg_id, 'power_neg_mean'] = power[power<0].mean()\n",
    "    X.loc[seg_id, 'power_neg_min'] = power[power<0].min()\n",
    "    X.loc[seg_id, 'power_neg_std'] = power[power<0].std()\n",
    "\n",
    "    bb = pd.Series(seg['Bearing'].values)\n",
    "    diff = np.diff(bb)\n",
    "    turn_power = np.multiply(diff,power[:-1]).values\n",
    "    X.loc[seg_id, 'turn_power_mean'] = turn_power.mean()\n",
    "     \n",
    "    return X\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bookingID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111669149733</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335007449205</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171798691856</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520418422900</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798863917116</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [111669149733, 335007449205, 171798691856, 1520418422900, 798863917116]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FEATURE ENGINEERING\n",
    "#Create a new training dataframe for storing newly-created features for each ride\n",
    "#Create a new label dataframe so that bookingID becomes index\n",
    "train_X_final_2 = pd.DataFrame(index=pd.Series(train_y.bookingID), dtype=np.float64)\n",
    "train_y_final = pd.DataFrame(index=pd.Series(train_y.bookingID),dtype=np.float64)\n",
    "train_y_final['label'] = train_y['label'].values\n",
    "train_X_final_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING(FOR TEST SET) \n",
    "#Create another new test set for test set summary stat features \n",
    "test_X_final_2 = pd.DataFrame(index=pd.Series(test_X_final.bookingID), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING\n",
    "#Sanity check to ensure no. of unique rides are the same in train set & label set\n",
    "assert len(train_X_final_2) == len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1048ea17320b4fdf88c15f6b4f69d4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19788), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:23: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:35: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:47: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:68: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:74: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:86: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:93: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:153: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:158: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:163: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:170: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:177: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:184: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:237: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:242: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:247: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:254: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:261: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:268: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:111: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:116: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:121: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:128: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:135: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:142: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:321: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:326: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:331: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:338: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:345: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:352: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:279: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:284: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:289: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:296: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:303: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:310: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:195: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:200: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:205: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:212: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:219: RuntimeWarning: Mean of empty slice.\n",
      "/home/khooweiquan2014/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:226: RuntimeWarning: Mean of empty slice.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#FEATURE ENGINEERING\n",
    "#Create features & store them to train_X_final_2\n",
    "#I didn't use parallel processing here since feature creation is quite fast(~1hour)\n",
    "for seg_id in tqdm(train_X_final.bookingID.unique()):\n",
    "    seg = train_X_final[train_X_final['bookingID'] == seg_id]\n",
    "    train_X_final_2 = create_features(seg_id, seg, train_X_final_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING(FOR TEST SET)\n",
    "#Create features & store them to test_X_final_2\n",
    "#I didn't use parallel processing here since feature creation is quite fast(~1hour)\n",
    "for seg_id in tqdm(test_X_final.bookingID.unique()):\n",
    "    seg = test_X_final[test_X_final['bookingID'] == seg_id]\n",
    "    test_X_final_2 = create_features(seg_id, seg, test_X_final_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING\n",
    "#Sanity check for inf values, which shouldn't exist\n",
    "for i in train_X_final_2.columns.unique():\n",
    "    if np.isinf(train_X_final_2[i]).any() == True:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING(FOR TEST SET)\n",
    "#Sanity check for inf values, which shouldn't exist\n",
    "for i in test_X_final_2.columns.unique():\n",
    "    if np.isinf(test_X_final_2[i]).any() == True:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_Speed</th>\n",
       "      <th>rolling_speed_2_std</th>\n",
       "      <th>hard_braking_speed_min</th>\n",
       "      <th>hard_braking_speed_std</th>\n",
       "      <th>hard_braking_speed_ptp</th>\n",
       "      <th>hard_braking_speed_num_peaks_10</th>\n",
       "      <th>hard_braking_speed_be_5</th>\n",
       "      <th>hard_acc_speed_max</th>\n",
       "      <th>hard_acc_speed_std</th>\n",
       "      <th>hard_acc_speed_ptp</th>\n",
       "      <th>...</th>\n",
       "      <th>power_max</th>\n",
       "      <th>power_min</th>\n",
       "      <th>power_med</th>\n",
       "      <th>power_pos_mean</th>\n",
       "      <th>power_pos_max</th>\n",
       "      <th>power_pos_std</th>\n",
       "      <th>power_neg_mean</th>\n",
       "      <th>power_neg_min</th>\n",
       "      <th>power_neg_std</th>\n",
       "      <th>turn_power_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.116691e+11</th>\n",
       "      <td>5.221561</td>\n",
       "      <td>0.303947</td>\n",
       "      <td>-2.966065</td>\n",
       "      <td>0.587765</td>\n",
       "      <td>2.963853</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.184315</td>\n",
       "      <td>2.610508</td>\n",
       "      <td>0.545873</td>\n",
       "      <td>2.597740</td>\n",
       "      <td>...</td>\n",
       "      <td>203.448537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.440804</td>\n",
       "      <td>87.934016</td>\n",
       "      <td>203.448537</td>\n",
       "      <td>45.742945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-24.572235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.350074e+11</th>\n",
       "      <td>6.029151</td>\n",
       "      <td>0.291928</td>\n",
       "      <td>-4.829653</td>\n",
       "      <td>0.655941</td>\n",
       "      <td>4.828535</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.597932</td>\n",
       "      <td>2.952679</td>\n",
       "      <td>0.538180</td>\n",
       "      <td>2.950594</td>\n",
       "      <td>...</td>\n",
       "      <td>204.180021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.796707</td>\n",
       "      <td>85.296985</td>\n",
       "      <td>204.180021</td>\n",
       "      <td>52.018306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.206442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.717987e+11</th>\n",
       "      <td>16.362283</td>\n",
       "      <td>0.127965</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>0.272192</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.212654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169775</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>...</td>\n",
       "      <td>254.795443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.251697</td>\n",
       "      <td>167.657437</td>\n",
       "      <td>254.795443</td>\n",
       "      <td>65.724623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.331214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.520418e+12</th>\n",
       "      <td>13.628550</td>\n",
       "      <td>0.282835</td>\n",
       "      <td>-2.630000</td>\n",
       "      <td>0.466653</td>\n",
       "      <td>2.620000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.862097</td>\n",
       "      <td>3.120000</td>\n",
       "      <td>0.412678</td>\n",
       "      <td>3.110000</td>\n",
       "      <td>...</td>\n",
       "      <td>293.526642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.391346</td>\n",
       "      <td>149.991389</td>\n",
       "      <td>293.526642</td>\n",
       "      <td>73.986091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-30.611714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.988639e+11</th>\n",
       "      <td>8.729576</td>\n",
       "      <td>0.363284</td>\n",
       "      <td>-2.564959</td>\n",
       "      <td>0.548929</td>\n",
       "      <td>2.562250</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.273726</td>\n",
       "      <td>4.337577</td>\n",
       "      <td>0.616003</td>\n",
       "      <td>4.337299</td>\n",
       "      <td>...</td>\n",
       "      <td>238.407435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.061418</td>\n",
       "      <td>115.829375</td>\n",
       "      <td>238.407435</td>\n",
       "      <td>48.175361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.097101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              mean_Speed  rolling_speed_2_std  hard_braking_speed_min  \\\n",
       "1.116691e+11    5.221561             0.303947               -2.966065   \n",
       "3.350074e+11    6.029151             0.291928               -4.829653   \n",
       "1.717987e+11   16.362283             0.127965               -2.750000   \n",
       "1.520418e+12   13.628550             0.282835               -2.630000   \n",
       "7.988639e+11    8.729576             0.363284               -2.564959   \n",
       "\n",
       "              hard_braking_speed_std  hard_braking_speed_ptp  \\\n",
       "1.116691e+11                0.587765                2.963853   \n",
       "3.350074e+11                0.655941                4.828535   \n",
       "1.717987e+11                0.272192                2.700000   \n",
       "1.520418e+12                0.466653                2.620000   \n",
       "7.988639e+11                0.548929                2.562250   \n",
       "\n",
       "              hard_braking_speed_num_peaks_10  hard_braking_speed_be_5  \\\n",
       "1.116691e+11                              9.0                 1.184315   \n",
       "3.350074e+11                             17.0                 0.597932   \n",
       "1.717987e+11                              0.0                 0.212654   \n",
       "1.520418e+12                             17.0                 0.862097   \n",
       "7.988639e+11                              9.0                 1.273726   \n",
       "\n",
       "              hard_acc_speed_max  hard_acc_speed_std  hard_acc_speed_ptp  \\\n",
       "1.116691e+11            2.610508            0.545873            2.597740   \n",
       "3.350074e+11            2.952679            0.538180            2.950594   \n",
       "1.717987e+11            1.000000            0.169775            0.950000   \n",
       "1.520418e+12            3.120000            0.412678            3.110000   \n",
       "7.988639e+11            4.337577            0.616003            4.337299   \n",
       "\n",
       "                   ...          power_max  power_min   power_med  \\\n",
       "1.116691e+11       ...         203.448537        0.0   34.440804   \n",
       "3.350074e+11       ...         204.180021        0.0   35.796707   \n",
       "1.717987e+11       ...         254.795443        0.0  187.251697   \n",
       "1.520418e+12       ...         293.526642        0.0  168.391346   \n",
       "7.988639e+11       ...         238.407435        0.0   95.061418   \n",
       "\n",
       "              power_pos_mean  power_pos_max  power_pos_std  power_neg_mean  \\\n",
       "1.116691e+11       87.934016     203.448537      45.742945             NaN   \n",
       "3.350074e+11       85.296985     204.180021      52.018306             NaN   \n",
       "1.717987e+11      167.657437     254.795443      65.724623             NaN   \n",
       "1.520418e+12      149.991389     293.526642      73.986091             NaN   \n",
       "7.988639e+11      115.829375     238.407435      48.175361             NaN   \n",
       "\n",
       "              power_neg_min  power_neg_std  turn_power_mean  \n",
       "1.116691e+11            NaN            NaN       -24.572235  \n",
       "3.350074e+11            NaN            NaN        14.206442  \n",
       "1.717987e+11            NaN            NaN        -1.331214  \n",
       "1.520418e+12            NaN            NaN       -30.611714  \n",
       "7.988639e+11            NaN            NaN         3.097101  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FEATURE ENGINEERING\n",
    "#Check first 5 rows of final training set\n",
    "train_X_final_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING(FOR TEST SET)\n",
    "test_X_final_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING\n",
    "#Check first 5 rows of train y\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING\n",
    "#Use StandardScaler to scale training set features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X_final_2)\n",
    "scaled_train_X = pd.DataFrame(scaler.transform(train_X_final_2), columns=train_X_final_2.columns,index=train_X_final_2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING(FOR TEST SET)\n",
    "#Use StandardScaler to scale test set features \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(test_X_final_2)\n",
    "scaled_test_X = pd.DataFrame(scaler.transform(test_X_final_2), columns=test_X_final_2.columns,index=test_X_final_2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING\n",
    "#Assign label values to train_y_final_2 for use in Lightgbm model later\n",
    "train_y_final_2 = train_y_final['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19788, 260)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FEATURE ENGINEERING\n",
    "#Checking no. of rows & columns for scaled_train_X, the final training set containing all the scaled features\n",
    "scaled_train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING(FOR TEST SET)\n",
    "#Checking no. of rows & columns for scaled_test_X, the final test set containing all the scaled features\n",
    "scaled_test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELING\n",
    "#Due to imbalanced class, I decided to do data augmentation within each fold by up-sampling both classes(but up-sampling more of class 1)\n",
    "def augment(input_X,input_y):\n",
    "    #Triples minor category\n",
    "    mask = input_y>0\n",
    "    x1 = input_X[mask].copy()     \n",
    "    new_input_X = input_X.append(x1.sample(frac=1))\n",
    "    new_input_X = new_input_X.append(x1.sample(frac=1))\n",
    "    new_input_y = input_y.append(input_y[mask])\n",
    "    new_input_y = new_input_y.append(input_y[mask])\n",
    "    \n",
    "    #Doubles major category\n",
    "    mask = input_y==0\n",
    "    x1 = input_X[mask].copy()\n",
    "    new_input_X = new_input_X.append(x1.sample(frac=1))\n",
    "    new_input_y = new_input_y.append(input_y[mask])\n",
    "    \n",
    "    return new_input_X,new_input_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELING\n",
    "folds = 5\n",
    "random_state = 0\n",
    "skf = StratifiedKFold(n_splits=folds,shuffle=True,random_state=random_state)\n",
    "#kf = KFold(n_splits=folds,shuffle=True,random_state=random_state)\n",
    "predictions = np.zeros(len(scaled_test_X))\n",
    "#scores = []\n",
    "#train_columns = scaled_train_X.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELING\n",
    "#Model paramters are as shown\n",
    "#I played around with the hyperparameters until a decent gap between train & val scores is achieved\n",
    "#Of course, feel free to further-tweak the model to improve model accuracy\n",
    "params = {\n",
    "    \"objective\" : \"binary\",\n",
    "    \"metric\" : \"auc\",\n",
    "    \"boosting\": 'gbdt',\n",
    "    \"max_depth\" : -1,\n",
    "    \"num_leaves\" :13,\n",
    "    \"num_threads\" : 8,\n",
    "    \"learning_rate\" : 0.01,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"bagging_fraction\" : 0.4,\n",
    "    \"feature_fraction\" : 0.01,\n",
    "    #\"min_data_in_leaf\": 300,\n",
    "    \"min_sum_hessian_in_leaf\" : 400,\n",
    "    \"tree_learner\": \"serial\",\n",
    "    \"boost_from_average\": \"false\",\n",
    "    \"lambda_l1\" : 5,\n",
    "    #\"lambda_l2\" : 5,\n",
    "    \"bagging_seed\" : random_state,\n",
    "    \"verbosity\" : -1,\n",
    "    \"seed\": random_state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Fold: 0\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.705564\tvalid_1's auc: 0.683635\n",
      "[2000]\ttraining's auc: 0.724518\tvalid_1's auc: 0.687954\n",
      "[3000]\ttraining's auc: 0.738379\tvalid_1's auc: 0.688739\n",
      "[4000]\ttraining's auc: 0.750191\tvalid_1's auc: 0.690299\n",
      "[5000]\ttraining's auc: 0.761015\tvalid_1's auc: 0.690898\n",
      "[6000]\ttraining's auc: 0.770436\tvalid_1's auc: 0.691181\n",
      "[7000]\ttraining's auc: 0.778805\tvalid_1's auc: 0.691404\n",
      "Early stopping, best iteration is:\n",
      "[6595]\ttraining's auc: 0.775489\tvalid_1's auc: 0.691598\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.705847\tvalid_1's auc: 0.683553\n",
      "[2000]\ttraining's auc: 0.725067\tvalid_1's auc: 0.687147\n",
      "[3000]\ttraining's auc: 0.739009\tvalid_1's auc: 0.689378\n",
      "[4000]\ttraining's auc: 0.750482\tvalid_1's auc: 0.69126\n",
      "[5000]\ttraining's auc: 0.761458\tvalid_1's auc: 0.691812\n",
      "[6000]\ttraining's auc: 0.770355\tvalid_1's auc: 0.692355\n",
      "[7000]\ttraining's auc: 0.778583\tvalid_1's auc: 0.69219\n",
      "[8000]\ttraining's auc: 0.786297\tvalid_1's auc: 0.691935\n",
      "Early stopping, best iteration is:\n",
      "[7427]\ttraining's auc: 0.781884\tvalid_1's auc: 0.692537\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.705811\tvalid_1's auc: 0.682007\n",
      "[2000]\ttraining's auc: 0.725104\tvalid_1's auc: 0.686732\n",
      "[3000]\ttraining's auc: 0.738988\tvalid_1's auc: 0.688525\n",
      "[4000]\ttraining's auc: 0.750675\tvalid_1's auc: 0.690192\n",
      "[5000]\ttraining's auc: 0.761562\tvalid_1's auc: 0.691119\n",
      "Early stopping, best iteration is:\n",
      "[4758]\ttraining's auc: 0.759093\tvalid_1's auc: 0.691262\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.706049\tvalid_1's auc: 0.68343\n",
      "[2000]\ttraining's auc: 0.725059\tvalid_1's auc: 0.68727\n",
      "[3000]\ttraining's auc: 0.739042\tvalid_1's auc: 0.689224\n",
      "[4000]\ttraining's auc: 0.750706\tvalid_1's auc: 0.69066\n",
      "[5000]\ttraining's auc: 0.761509\tvalid_1's auc: 0.691804\n",
      "[6000]\ttraining's auc: 0.770531\tvalid_1's auc: 0.692026\n",
      "[7000]\ttraining's auc: 0.778762\tvalid_1's auc: 0.691442\n",
      "Early stopping, best iteration is:\n",
      "[6005]\ttraining's auc: 0.770526\tvalid_1's auc: 0.692076\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.706001\tvalid_1's auc: 0.683751\n",
      "[2000]\ttraining's auc: 0.724647\tvalid_1's auc: 0.688004\n",
      "[3000]\ttraining's auc: 0.73855\tvalid_1's auc: 0.689705\n",
      "[4000]\ttraining's auc: 0.750145\tvalid_1's auc: 0.691034\n",
      "[5000]\ttraining's auc: 0.760895\tvalid_1's auc: 0.691985\n",
      "[6000]\ttraining's auc: 0.770336\tvalid_1's auc: 0.692258\n",
      "[7000]\ttraining's auc: 0.778534\tvalid_1's auc: 0.692559\n",
      "Early stopping, best iteration is:\n",
      "[6746]\ttraining's auc: 0.776439\tvalid_1's auc: 0.69277\n",
      "Current Fold: 1\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.70658\tvalid_1's auc: 0.674667\n",
      "[2000]\ttraining's auc: 0.725353\tvalid_1's auc: 0.682936\n",
      "[3000]\ttraining's auc: 0.739188\tvalid_1's auc: 0.685817\n",
      "[4000]\ttraining's auc: 0.750865\tvalid_1's auc: 0.688019\n",
      "[5000]\ttraining's auc: 0.761806\tvalid_1's auc: 0.688269\n",
      "[6000]\ttraining's auc: 0.770702\tvalid_1's auc: 0.688777\n",
      "[7000]\ttraining's auc: 0.779141\tvalid_1's auc: 0.689106\n",
      "Early stopping, best iteration is:\n",
      "[6759]\ttraining's auc: 0.777379\tvalid_1's auc: 0.68942\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.706532\tvalid_1's auc: 0.675239\n",
      "[2000]\ttraining's auc: 0.726414\tvalid_1's auc: 0.6836\n",
      "[3000]\ttraining's auc: 0.739652\tvalid_1's auc: 0.685838\n",
      "[4000]\ttraining's auc: 0.751244\tvalid_1's auc: 0.687495\n",
      "[5000]\ttraining's auc: 0.762258\tvalid_1's auc: 0.687395\n",
      "Early stopping, best iteration is:\n",
      "[4307]\ttraining's auc: 0.75443\tvalid_1's auc: 0.687855\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.706024\tvalid_1's auc: 0.676063\n",
      "[2000]\ttraining's auc: 0.725503\tvalid_1's auc: 0.683203\n",
      "[3000]\ttraining's auc: 0.739255\tvalid_1's auc: 0.686487\n",
      "[4000]\ttraining's auc: 0.751038\tvalid_1's auc: 0.68881\n",
      "[5000]\ttraining's auc: 0.762237\tvalid_1's auc: 0.688436\n",
      "Early stopping, best iteration is:\n",
      "[4307]\ttraining's auc: 0.754455\tvalid_1's auc: 0.689239\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.706531\tvalid_1's auc: 0.675053\n",
      "[2000]\ttraining's auc: 0.726024\tvalid_1's auc: 0.683056\n",
      "[3000]\ttraining's auc: 0.739559\tvalid_1's auc: 0.686241\n",
      "[4000]\ttraining's auc: 0.751053\tvalid_1's auc: 0.68859\n",
      "[5000]\ttraining's auc: 0.761907\tvalid_1's auc: 0.688575\n",
      "Early stopping, best iteration is:\n",
      "[4307]\ttraining's auc: 0.754083\tvalid_1's auc: 0.688957\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.707076\tvalid_1's auc: 0.676501\n",
      "[2000]\ttraining's auc: 0.726161\tvalid_1's auc: 0.685174\n",
      "[3000]\ttraining's auc: 0.739958\tvalid_1's auc: 0.687817\n",
      "[4000]\ttraining's auc: 0.751152\tvalid_1's auc: 0.689853\n",
      "[5000]\ttraining's auc: 0.762226\tvalid_1's auc: 0.689599\n",
      "Early stopping, best iteration is:\n",
      "[4039]\ttraining's auc: 0.751532\tvalid_1's auc: 0.69007\n",
      "Current Fold: 2\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.706611\tvalid_1's auc: 0.6682\n",
      "[2000]\ttraining's auc: 0.726841\tvalid_1's auc: 0.676425\n",
      "[3000]\ttraining's auc: 0.741054\tvalid_1's auc: 0.677644\n",
      "[4000]\ttraining's auc: 0.75266\tvalid_1's auc: 0.677828\n",
      "[5000]\ttraining's auc: 0.763273\tvalid_1's auc: 0.678049\n",
      "Early stopping, best iteration is:\n",
      "[4618]\ttraining's auc: 0.759527\tvalid_1's auc: 0.678515\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.707181\tvalid_1's auc: 0.668928\n",
      "[2000]\ttraining's auc: 0.727228\tvalid_1's auc: 0.676284\n",
      "[3000]\ttraining's auc: 0.741007\tvalid_1's auc: 0.67743\n",
      "[4000]\ttraining's auc: 0.752461\tvalid_1's auc: 0.678129\n",
      "[5000]\ttraining's auc: 0.763059\tvalid_1's auc: 0.67907\n",
      "[6000]\ttraining's auc: 0.771929\tvalid_1's auc: 0.679188\n",
      "[7000]\ttraining's auc: 0.779667\tvalid_1's auc: 0.678896\n",
      "Early stopping, best iteration is:\n",
      "[6207]\ttraining's auc: 0.773469\tvalid_1's auc: 0.67939\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.706537\tvalid_1's auc: 0.670109\n",
      "[2000]\ttraining's auc: 0.726408\tvalid_1's auc: 0.676387\n",
      "[3000]\ttraining's auc: 0.740485\tvalid_1's auc: 0.677772\n",
      "[4000]\ttraining's auc: 0.751811\tvalid_1's auc: 0.679033\n",
      "[5000]\ttraining's auc: 0.762538\tvalid_1's auc: 0.678813\n",
      "Early stopping, best iteration is:\n",
      "[4624]\ttraining's auc: 0.758912\tvalid_1's auc: 0.679295\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.706887\tvalid_1's auc: 0.666848\n",
      "[2000]\ttraining's auc: 0.726642\tvalid_1's auc: 0.675019\n",
      "[3000]\ttraining's auc: 0.740789\tvalid_1's auc: 0.677082\n",
      "[4000]\ttraining's auc: 0.752265\tvalid_1's auc: 0.678004\n",
      "[5000]\ttraining's auc: 0.762679\tvalid_1's auc: 0.678003\n",
      "Early stopping, best iteration is:\n",
      "[4559]\ttraining's auc: 0.758211\tvalid_1's auc: 0.678575\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.707201\tvalid_1's auc: 0.668726\n",
      "[2000]\ttraining's auc: 0.727278\tvalid_1's auc: 0.67611\n",
      "[3000]\ttraining's auc: 0.741033\tvalid_1's auc: 0.677531\n",
      "[4000]\ttraining's auc: 0.75268\tvalid_1's auc: 0.677907\n",
      "Early stopping, best iteration is:\n",
      "[3660]\ttraining's auc: 0.749001\tvalid_1's auc: 0.678262\n",
      "Current Fold: 3\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.708205\tvalid_1's auc: 0.663528\n",
      "[2000]\ttraining's auc: 0.726451\tvalid_1's auc: 0.67195\n",
      "[3000]\ttraining's auc: 0.739479\tvalid_1's auc: 0.674522\n",
      "[4000]\ttraining's auc: 0.750829\tvalid_1's auc: 0.67718\n",
      "[5000]\ttraining's auc: 0.761371\tvalid_1's auc: 0.679507\n",
      "[6000]\ttraining's auc: 0.770491\tvalid_1's auc: 0.680413\n",
      "[7000]\ttraining's auc: 0.778409\tvalid_1's auc: 0.68088\n",
      "Early stopping, best iteration is:\n",
      "[6783]\ttraining's auc: 0.776722\tvalid_1's auc: 0.681246\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.708102\tvalid_1's auc: 0.662829\n",
      "[2000]\ttraining's auc: 0.726059\tvalid_1's auc: 0.671878\n",
      "[3000]\ttraining's auc: 0.739776\tvalid_1's auc: 0.674812\n",
      "[4000]\ttraining's auc: 0.751243\tvalid_1's auc: 0.677939\n",
      "[5000]\ttraining's auc: 0.761705\tvalid_1's auc: 0.679959\n",
      "[6000]\ttraining's auc: 0.770942\tvalid_1's auc: 0.680699\n",
      "[7000]\ttraining's auc: 0.778797\tvalid_1's auc: 0.681711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8000]\ttraining's auc: 0.786207\tvalid_1's auc: 0.682254\n",
      "Early stopping, best iteration is:\n",
      "[7844]\ttraining's auc: 0.785002\tvalid_1's auc: 0.68245\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.707487\tvalid_1's auc: 0.662126\n",
      "[2000]\ttraining's auc: 0.726289\tvalid_1's auc: 0.671204\n",
      "[3000]\ttraining's auc: 0.73937\tvalid_1's auc: 0.674053\n",
      "[4000]\ttraining's auc: 0.751306\tvalid_1's auc: 0.677653\n",
      "[5000]\ttraining's auc: 0.761697\tvalid_1's auc: 0.679082\n",
      "[6000]\ttraining's auc: 0.770949\tvalid_1's auc: 0.679989\n",
      "[7000]\ttraining's auc: 0.778908\tvalid_1's auc: 0.68096\n",
      "[8000]\ttraining's auc: 0.786286\tvalid_1's auc: 0.681112\n",
      "Early stopping, best iteration is:\n",
      "[7835]\ttraining's auc: 0.785133\tvalid_1's auc: 0.681432\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.707768\tvalid_1's auc: 0.664176\n",
      "[2000]\ttraining's auc: 0.726327\tvalid_1's auc: 0.672015\n",
      "[3000]\ttraining's auc: 0.739863\tvalid_1's auc: 0.674995\n",
      "[4000]\ttraining's auc: 0.751396\tvalid_1's auc: 0.678241\n",
      "[5000]\ttraining's auc: 0.761856\tvalid_1's auc: 0.679998\n",
      "[6000]\ttraining's auc: 0.770883\tvalid_1's auc: 0.680735\n",
      "[7000]\ttraining's auc: 0.778727\tvalid_1's auc: 0.680797\n",
      "Early stopping, best iteration is:\n",
      "[6782]\ttraining's auc: 0.77708\tvalid_1's auc: 0.6813\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.707279\tvalid_1's auc: 0.663331\n",
      "[2000]\ttraining's auc: 0.726151\tvalid_1's auc: 0.672095\n",
      "[3000]\ttraining's auc: 0.739488\tvalid_1's auc: 0.675098\n",
      "[4000]\ttraining's auc: 0.750946\tvalid_1's auc: 0.678016\n",
      "[5000]\ttraining's auc: 0.761374\tvalid_1's auc: 0.679524\n",
      "[6000]\ttraining's auc: 0.770713\tvalid_1's auc: 0.68091\n",
      "[7000]\ttraining's auc: 0.778763\tvalid_1's auc: 0.681254\n",
      "Early stopping, best iteration is:\n",
      "[6681]\ttraining's auc: 0.776072\tvalid_1's auc: 0.68161\n",
      "Current Fold: 4\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.70648\tvalid_1's auc: 0.676382\n",
      "[2000]\ttraining's auc: 0.725379\tvalid_1's auc: 0.68502\n",
      "[3000]\ttraining's auc: 0.739357\tvalid_1's auc: 0.687644\n",
      "[4000]\ttraining's auc: 0.751099\tvalid_1's auc: 0.689883\n",
      "[5000]\ttraining's auc: 0.762251\tvalid_1's auc: 0.691135\n",
      "[6000]\ttraining's auc: 0.770878\tvalid_1's auc: 0.691008\n",
      "[7000]\ttraining's auc: 0.7784\tvalid_1's auc: 0.691014\n",
      "Early stopping, best iteration is:\n",
      "[6276]\ttraining's auc: 0.773029\tvalid_1's auc: 0.691475\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.707141\tvalid_1's auc: 0.67612\n",
      "[2000]\ttraining's auc: 0.725924\tvalid_1's auc: 0.684333\n",
      "[3000]\ttraining's auc: 0.739561\tvalid_1's auc: 0.687384\n",
      "[4000]\ttraining's auc: 0.751187\tvalid_1's auc: 0.689765\n",
      "[5000]\ttraining's auc: 0.762015\tvalid_1's auc: 0.690902\n",
      "Early stopping, best iteration is:\n",
      "[4981]\ttraining's auc: 0.761784\tvalid_1's auc: 0.690991\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.70607\tvalid_1's auc: 0.675671\n",
      "[2000]\ttraining's auc: 0.725426\tvalid_1's auc: 0.684164\n",
      "[3000]\ttraining's auc: 0.739421\tvalid_1's auc: 0.68726\n",
      "[4000]\ttraining's auc: 0.751191\tvalid_1's auc: 0.689434\n",
      "[5000]\ttraining's auc: 0.761687\tvalid_1's auc: 0.691091\n",
      "[6000]\ttraining's auc: 0.770628\tvalid_1's auc: 0.691206\n",
      "Early stopping, best iteration is:\n",
      "[5962]\ttraining's auc: 0.770334\tvalid_1's auc: 0.691303\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.70658\tvalid_1's auc: 0.675809\n",
      "[2000]\ttraining's auc: 0.725548\tvalid_1's auc: 0.684946\n",
      "[3000]\ttraining's auc: 0.739644\tvalid_1's auc: 0.687868\n",
      "[4000]\ttraining's auc: 0.75143\tvalid_1's auc: 0.689589\n",
      "[5000]\ttraining's auc: 0.761892\tvalid_1's auc: 0.691458\n",
      "[6000]\ttraining's auc: 0.77066\tvalid_1's auc: 0.692003\n",
      "[7000]\ttraining's auc: 0.77832\tvalid_1's auc: 0.691664\n",
      "Early stopping, best iteration is:\n",
      "[6191]\ttraining's auc: 0.772007\tvalid_1's auc: 0.692241\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's auc: 0.706349\tvalid_1's auc: 0.677055\n",
      "[2000]\ttraining's auc: 0.725273\tvalid_1's auc: 0.685465\n",
      "[3000]\ttraining's auc: 0.740017\tvalid_1's auc: 0.688534\n",
      "[4000]\ttraining's auc: 0.751368\tvalid_1's auc: 0.689842\n",
      "[5000]\ttraining's auc: 0.762054\tvalid_1's auc: 0.691745\n",
      "[6000]\ttraining's auc: 0.770831\tvalid_1's auc: 0.692125\n",
      "[7000]\ttraining's auc: 0.778643\tvalid_1's auc: 0.691556\n",
      "Early stopping, best iteration is:\n",
      "[6263]\ttraining's auc: 0.772865\tvalid_1's auc: 0.692373\n"
     ]
    }
   ],
   "source": [
    "#MODELING\n",
    "#I'm using Lightgbm since I'm most-comfortable with this model(after using it for kaggle competitions & achieving decent results)\n",
    "#Online sources suggested LSTM as well. Due to time constraints however, I'm not doing a blend of different models\n",
    "#Within each fold, I augment 5 times. 25 times in total\n",
    "feature_importance = pd.DataFrame()\n",
    "yp_final = 0\n",
    "for fold_, (trn_idx, val_idx) in enumerate(skf.split(scaled_train_X,train_y_final_2)):\n",
    "    \n",
    "    \n",
    "    print(\"Current Fold: {}\".format(fold_))\n",
    "    input_train = scaled_train_X.iloc[trn_idx]\n",
    "    target_train = train_y_final_2.iloc[trn_idx]\n",
    "    N = 5\n",
    "    yp = 0\n",
    "    for i in range(N):\n",
    "        auginput_train,augtarget_train = augment(input_train,target_train)\n",
    "        trn_data = lgb.Dataset(auginput_train, label=augtarget_train)\n",
    "        val_data = lgb.Dataset(scaled_train_X.iloc[val_idx], label=train_y_final_2.iloc[val_idx])\n",
    "        evals_result = {}\n",
    "        model = lgb.train(params,trn_data,100000,valid_sets = [trn_data, val_data],early_stopping_rounds=1000,verbose_eval = 1000,evals_result=evals_result)                   \n",
    "        yp += model.predict(scaled_test_X)     \n",
    "          \n",
    "    yp_final += (yp/N)\n",
    "predictions = yp_final/folds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7558901902021601"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MODELING\n",
    "#A gauge of how our model performs overall on the training set\n",
    "#Of course, this is a slightly-inaccurate gauge since there is definitely leakage between training set & 'test set'(which is also training set)\n",
    "roc_auc_score(train_y_final_2, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-6f105019eed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#This is better than the default feature_importances_ of lightgbm since the lightgbm version uses no. of splits to judge a feature's importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_train_X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreeExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_importance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_importance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shap' is not defined"
     ]
    }
   ],
   "source": [
    "#FEATURE IMPORTANCES\n",
    "#Use shap to plot bar-chart that shows feature importances in descending order\n",
    "#This is better than the default feature_importances_ of lightgbm since the lightgbm version uses no. of splits to judge a feature's importance\n",
    "X_importance = scaled_train_X\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_importance)\n",
    "shap.summary_plot(shap_values, X_importance, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE IMPORTANCES\n",
    "#Can drop features based on shap values\n",
    "#However, doing this seems to produce erratic results so I shall skip this\n",
    "\n",
    "#shap_sum = np.abs(shap_values).mean(axis=0)\n",
    "#importance_df = pd.DataFrame([scaled_train_X.columns.tolist(), shap_sum.tolist()]).T\n",
    "#importance_df.columns = ['column_name', 'shap_importance']\n",
    "#importance_df = importance_df.sort_values('shap_importance', ascending=False)\n",
    "#to_remove = importance_df[importance_df['shap_importance'] < 0.001]\n",
    "#to_drop = to_remove['column_name'].tolist()\n",
    "#scaled_train_X_filtered = scaled_train_X.drop((i for i in to_drop),axis=1)\n",
    "#scaled_test_X_filtered = scaled_test_X.drop((i for i in to_drop),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookingID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.116691e+11</td>\n",
       "      <td>0.264637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.350074e+11</td>\n",
       "      <td>0.408246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.717987e+11</td>\n",
       "      <td>0.075052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.520418e+12</td>\n",
       "      <td>0.281480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.988639e+11</td>\n",
       "      <td>0.282884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bookingID     label\n",
       "0  1.116691e+11  0.264637\n",
       "1  3.350074e+11  0.408246\n",
       "2  1.717987e+11  0.075052\n",
       "3  1.520418e+12  0.281480\n",
       "4  7.988639e+11  0.282884"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SUBMISSION\n",
    "#Save predictions to csv file\n",
    "test_pred =pd.DataFrame(dtype=np.float64)\n",
    "test_pred['bookingID'] = test_X_final_2.index\n",
    "test_pred['label'] = predictions\n",
    "test_pred.to_csv('predictions.csv', index=False)\n",
    "test_pred.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
